{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "leily.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B-sF8y41oiu-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import math\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv', sep='\\t')\n",
        "train = train.head(2000)\n",
        "train['text'] = train['text'].astype(str)\n",
        "\n",
        "test = pd.read_csv('test.csv', sep='\\t')\n",
        "test = test.head(200)\n",
        "test['text'] = test['text'].astype(str)"
      ],
      "metadata": {
        "id": "DY139LavAumg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def addCharFirst(text,char,place):\n",
        "    return text[:place] + char + text[place:]\n",
        "\n",
        "def addCharEnd(text,char,place):\n",
        "    return text[:place] + char + text[place:]\n",
        "\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "def get_batches(arr, n_seqs, n_steps):\n",
        "    batch_size = n_seqs * n_steps\n",
        "    n_batches = len(arr)//batch_size\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size]\n",
        "    \n",
        "    # Reshape into n_seqs rows\n",
        "    arr = arr.reshape((n_seqs, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], n_steps):\n",
        "        \n",
        "        # The features\n",
        "        x = arr[:, n:n+n_steps]\n",
        "        \n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        \n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "\n",
        "def levenshtein(seq1, seq2):\n",
        "    size_x = len(seq1) + 1\n",
        "    size_y = len(seq2) + 1\n",
        "    matrix = np.zeros ((size_x, size_y))\n",
        "    for x in range(size_x):\n",
        "        matrix [x, 0] = x\n",
        "    for y in range(size_y):\n",
        "        matrix [0, y] = y\n",
        "\n",
        "    for x in range(1, size_x):\n",
        "        for y in range(1, size_y):\n",
        "            if seq1[x-1] == seq2[y-1]:\n",
        "                matrix [x,y] = min(\n",
        "                    matrix[x-1, y] + 1,\n",
        "                    matrix[x-1, y-1],\n",
        "                    matrix[x, y-1] + 1\n",
        "                )\n",
        "            else:\n",
        "                matrix [x,y] = min(\n",
        "                    matrix[x-1,y] + 1,\n",
        "                    matrix[x-1,y-1] + 1,\n",
        "                    matrix[x,y-1] + 1\n",
        "                )\n",
        "\n",
        "    return (matrix[size_x - 1, size_y - 1])"
      ],
      "metadata": {
        "id": "XQ_f_koNSuEl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PreProcessor Class"
      ],
      "metadata": {
        "id": "dcQFREkO0JY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor():\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.data = data\n",
        "        self.clean()\n",
        "\n",
        "        self.text = ''\n",
        "\n",
        "        for idx in range(len(self.data.index)):\n",
        "            self.text = self.text + self.data.iloc[idx]['text']\n",
        "            self.text = self.text + '\\n\\n'\n",
        "\n",
        "        self.count_chars()\n",
        "        self.tokenize()\n",
        "    \n",
        "    \n",
        "    def clean(self):\n",
        "        # remove english alphabet\n",
        "        def tmp1(txt):\n",
        "            txt = re.sub(r'[a-zA-Z]+', ' ', txt)\n",
        "            return txt\n",
        "        # replace numbers with 'N'\n",
        "        def tmp2(txt):\n",
        "            # persian numbers\n",
        "            new_res = re.sub('[\\u06F0-\\u06F9]+', 'N', txt)\n",
        "            # english numbers\n",
        "            new_res2 = re.sub(r\"\\d+\", 'N', new_res)\n",
        "            return new_res2\n",
        "        def tmp3(txt):\n",
        "            txt = txt.replace(\"؟\", \"adgg\")\n",
        "            txt = txt.replace(\".\", \"adgn\")\n",
        "            txt = re.sub(r'[^\\w]', ' ', txt)\n",
        "            txt = txt.replace(\"adgg\",   \"؟\")\n",
        "            txt = txt.replace(\"adgn\", \".\")\n",
        "            return txt\n",
        "        # convert multiple consecutive spaces into one space\n",
        "        def tmp4(txt):\n",
        "            return ' '.join(txt.split())\n",
        "        # add special chars to start and end of a string\n",
        "        def tmp5(txt):\n",
        "            txt = addCharFirst(txt, ' \\e ', len(txt)+1)\n",
        "            txt = addCharEnd(txt, ' \\s ', 0)\n",
        "            return txt\n",
        "\n",
        "        self.data['text'] = self.data['text'].apply(tmp1)\n",
        "        self.data['text'] = self.data['text'].apply(tmp2)\n",
        "        self.data['text'] = self.data['text'].apply(tmp3)\n",
        "        self.data['text'] = self.data['text'].apply(tmp4)\n",
        "        self.data['text'] = self.data['text'].apply(tmp5)\n",
        "        \n",
        "    \n",
        "    def count_chars(self):\n",
        "        num_of_chars = len(self.text)\n",
        "\n",
        "        unique_chars = list(set(self.text))\n",
        "        \n",
        "        frequency = dict()\n",
        "        for char in unique_chars:\n",
        "            frequency[char] = self.text.count(char)\n",
        "        \n",
        "        print(\"Number of all chars = %s\" %(num_of_chars, ))\n",
        "        print(\"Frequency of each char:\")\n",
        "        print(frequency)\n",
        "\n",
        "    \n",
        "    def tokenize(self):\n",
        "        self.chars = tuple(set(self.text))\n",
        "\n",
        "        self.index2char = dict(enumerate(self.chars))\n",
        "        self.char2index = {ch: ii for ii, ch in self.index2char.items()}\n",
        "        # self.encoded = np.array([self.char2index[ch] for ch in self.text])\n",
        "\n",
        "        a_file = open(\"index2char.json\", \"w\")\n",
        "        json.dump(self.index2char, a_file)\n",
        "        a_file.close()\n",
        "\n",
        "        a_file = open(\"char2index.json\", \"w\")\n",
        "        json.dump(self.char2index, a_file)\n",
        "        a_file.close()\n",
        "\n",
        "    \n",
        "    def col2text(self):\n",
        "        return self.text"
      ],
      "metadata": {
        "id": "oeMcGM4azJNA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LanguageModel Class"
      ],
      "metadata": {
        "id": "7Z1L1M3W56BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, text, CUDA):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.top_k = 5\n",
        "        self.CUDA = CUDA\n",
        "\n",
        "        # Creating character dictionaries\n",
        "        self.chars = tuple(set(text))\n",
        "        self.index2char = dict(enumerate(self.chars))\n",
        "        self.char2index = {ch: ii for ii, ch in self.index2char.items()}\n",
        "        self.encoded = np.array([self.char2index[ch] for ch in text])\n",
        "\n",
        "        self.define_model(self.chars)\n",
        "    \n",
        "    \n",
        "    def define_model(self, tokens):\n",
        "        self.drop_prob = 0.5\n",
        "        self.n_layers = 2\n",
        "        self.n_hidden = 512\n",
        "        self.lr = 0.001\n",
        "        \n",
        "        ## Define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), self.n_hidden, self.n_layers, \n",
        "                            dropout=self.drop_prob, batch_first=True)\n",
        "        \n",
        "        ## Define a dropout layer\n",
        "        self.dropout = nn.Dropout(self.drop_prob)\n",
        "        \n",
        "        ## Define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(self.n_hidden, len(self.chars))\n",
        "        \n",
        "        # Initialize the weights\n",
        "        initrange = 0.1\n",
        "        \n",
        "        # Set bias tensor to all zeros\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        # FC weights as random uniform\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, hc):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hc`. '''\n",
        "        \n",
        "        ## Get x, and the new hidden state (h, c) from the lstm\n",
        "        x, (h, c) = self.lstm(x, hc)\n",
        "        \n",
        "        ## Ppass x through the dropout layer\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        x = x.reshape(x.size()[0]*x.size()[1], self.n_hidden)\n",
        "        \n",
        "        ## Put x through the fully-connected layer\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        # Return x and the hidden state (h, c)\n",
        "        return x, (h, c)\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, n_seqs):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
        "        \n",
        "    \n",
        "    def train_char_LSTM(self, epochs=20, n_seqs=10, n_steps=50, clip=5, val_frac=0.1, print_every=10):\n",
        "        self.net = LanguageModel(self.chars, self.CUDA)\n",
        "    \n",
        "        opt = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        # create training and validation data\n",
        "        val_idx = int(len(self.encoded)*(1-val_frac))\n",
        "        data, val_data = self.encoded[:val_idx], self.encoded[val_idx:]\n",
        "        \n",
        "        if self.CUDA:\n",
        "            self.net.cuda()\n",
        "        \n",
        "        counter = 0\n",
        "        n_chars = len(self.net.chars)\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            \n",
        "            h = self.net.init_hidden(n_seqs)\n",
        "            \n",
        "            for x, y in get_batches(data, n_seqs, n_steps):\n",
        "                \n",
        "                counter += 1\n",
        "                \n",
        "                # One-hot encode our data and make them Torch tensors\n",
        "                x = one_hot_encode(x, n_chars)\n",
        "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                \n",
        "                if self.CUDA:\n",
        "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                h = tuple([each.data for each in h])\n",
        "\n",
        "                self.net.zero_grad()\n",
        "                \n",
        "                output, h = self.net.forward(inputs, h)\n",
        "                \n",
        "                if self.CUDA:\n",
        "                    loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
        "                else:\n",
        "                    loss = criterion(output, targets.view(n_seqs*n_steps))\n",
        "\n",
        "                loss.backward()\n",
        "                \n",
        "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "                nn.utils.clip_grad_norm_(self.net.parameters(), clip)\n",
        "\n",
        "                opt.step()\n",
        "                \n",
        "                if counter % print_every == 0:\n",
        "                    \n",
        "                    # Get validation loss\n",
        "                    val_h = self.net.init_hidden(n_seqs)\n",
        "                    val_losses = []\n",
        "                    \n",
        "                    for x, y in get_batches(val_data, n_seqs, n_steps):\n",
        "                        \n",
        "                        # One-hot encode our data and make them Torch tensors\n",
        "                        x = one_hot_encode(x, n_chars)\n",
        "                        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                        \n",
        "                        # Creating new variables for the hidden state, otherwise\n",
        "                        # we'd backprop through the entire training history\n",
        "                        val_h = tuple([each.data for each in val_h])\n",
        "                        \n",
        "                        inputs, targets = x, y\n",
        "                        if self.CUDA:\n",
        "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                        output, val_h = self.net.forward(inputs, val_h)\n",
        "                        if self.CUDA:\n",
        "                            val_loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
        "                        else:\n",
        "                            val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
        "                        \n",
        "                        val_losses.append(val_loss.item())\n",
        "                    \n",
        "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                        \"Step: {}...\".format(counter),\n",
        "                        \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                        \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "    \n",
        "    \n",
        "    def get_next_states_and_output(self, char, h=None):\n",
        "        if self.CUDA:\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "        \n",
        "        if h is None:\n",
        "            h = self.init_hidden(1)\n",
        "        \n",
        "        x = np.array([[self.char2index[char]]])\n",
        "        x = one_hot_encode(x, len(self.chars))\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if self.CUDA:\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = self.forward(inputs, h)\n",
        "            \n",
        "        return out, h\n",
        "    \n",
        "    \n",
        "    def convert_prefix_to_hiddens(self, prefix):\n",
        "        if self.CUDA:\n",
        "            self.net.cuda()\n",
        "        else:\n",
        "            self.net.cpu()\n",
        "\n",
        "        self.net.eval()\n",
        "        \n",
        "        # First off, run through the prime characters\n",
        "        chars = [ch for ch in prefix]\n",
        "        \n",
        "        h = self.net.init_hidden(1)\n",
        "        \n",
        "        hidden_list = []\n",
        "\n",
        "        for ch in prefix:\n",
        "            char, h, _ = self.net.get_next_char(ch, h)\n",
        "            hidden_list.append(h)\n",
        "\n",
        "        return hidden_list\n",
        "    \n",
        "    \n",
        "    def get_probs(self, prefix):\n",
        "        if self.CUDA:\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "        \n",
        "        self.top_k = None\n",
        "                \n",
        "        self.net.eval()\n",
        "        \n",
        "        chars = [ch for ch in prefix]\n",
        "        \n",
        "        h = self.net.init_hidden(1)\n",
        "        \n",
        "        for ch in prefix:\n",
        "            x = np.array([[self.char2index[ch]]])\n",
        "            x = one_hot_encode(x, len(self.chars))\n",
        "            \n",
        "            inputs = torch.from_numpy(x)\n",
        "            \n",
        "            if self.CUDA:\n",
        "                inputs = inputs.cuda()\n",
        "            \n",
        "            h = tuple([each.data for each in h])\n",
        "            out, h = self.forward(inputs, h)\n",
        "\n",
        "            p = F.softmax(out, dim=1).data\n",
        "            \n",
        "            if self.CUDA:\n",
        "                p = p.cpu()\n",
        "            \n",
        "            top_ch = np.arange(len(self.chars))\n",
        "            \n",
        "            p = p.numpy().squeeze()\n",
        "        \n",
        "        self.top_k = 5\n",
        "\n",
        "        return dict(zip(self.chars, p))\n",
        "\n",
        "    \n",
        "    def get_next_char(self, char, h=None):\n",
        "        if self.CUDA:\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "        \n",
        "        if h is None:\n",
        "            h = self.init_hidden(1)\n",
        "        \n",
        "        x = np.array([[self.char2index[char]]])\n",
        "        x = one_hot_encode(x, len(self.chars))\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if self.CUDA:\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = self.forward(inputs, h)\n",
        "\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        \n",
        "        if self.CUDA:\n",
        "            p = p.cpu()\n",
        "        \n",
        "        if self.top_k is None:\n",
        "            top_ch = np.arange(len(self.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(self.top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        p = p.numpy().squeeze()\n",
        "        \n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "            \n",
        "        return self.index2char[char], h, p\n",
        "    \n",
        "    \n",
        "    def generate_text(self, prefix, size):\n",
        "        if self.CUDA:\n",
        "            self.net.cuda()\n",
        "        else:\n",
        "            self.net.cpu()\n",
        "\n",
        "        self.net.eval()\n",
        "        \n",
        "        # First off, run through the prime characters\n",
        "        chars = [ch for ch in prefix]\n",
        "        \n",
        "        h = self.net.init_hidden(1)\n",
        "        \n",
        "        for ch in prefix:\n",
        "            char, h, _ = self.net.get_next_char(ch, h)\n",
        "\n",
        "        chars.append(char)\n",
        "        \n",
        "        # Now pass in the previous character and get a new one\n",
        "        for ii in range(size-1):\n",
        "            \n",
        "            char, h, _ = self.net.get_next_char(chars[-1], h)\n",
        "            chars.append(char)\n",
        "\n",
        "        return ''.join(chars)\n",
        "\n",
        "    \n",
        "    def get_overal_prob(self, sentence):\n",
        "        def get_single_char_prob(char):\n",
        "            if self.CUDA:\n",
        "                self.cuda()\n",
        "            else:\n",
        "                self.cpu()\n",
        "            \n",
        "            h = self.init_hidden(1)\n",
        "            \n",
        "            x = np.array([[self.char2index[char]]])\n",
        "            x = one_hot_encode(x, len(self.chars))\n",
        "            \n",
        "            inputs = torch.from_numpy(x)\n",
        "            \n",
        "            if self.CUDA:\n",
        "                inputs = inputs.cuda()\n",
        "            \n",
        "            h = tuple([each.data for each in h])\n",
        "            out, h = self.forward(inputs, h)\n",
        "\n",
        "            p = F.softmax(out, dim=1).data\n",
        "            \n",
        "            if self.CUDA:\n",
        "                p = p.cpu()\n",
        "            \n",
        "            top_ch = np.arange(len(self.chars))\n",
        "            \n",
        "            p = p.numpy().squeeze()\n",
        "\n",
        "            return p\n",
        "        \n",
        "        overal_prob = 0\n",
        "        current_char = sentence[0]\n",
        "\n",
        "        for char in sentence[1:]:\n",
        "            probs = get_single_char_prob(current_char)\n",
        "            probabily = probs[self.char2index[char]]\n",
        "            overal_prob += math.log2(probabily)\n",
        "            current_char = char\n",
        "\n",
        "        return overal_prob\n",
        "\n",
        "    \n",
        "    def evaluate(self, ground_truth):\n",
        "        # get first five words of the ground_truth\n",
        "        first_five = ' '.join(ground_truth.split()[:5])\n",
        "\n",
        "        # calculate the number of characters to be generated\n",
        "        size_to_generate = len(ground_truth) - len(first_five)\n",
        "\n",
        "        generated_text = self.generate_text(prefix=first_five, size=size_to_generate)\n",
        "\n",
        "        # calculate the character error rate\n",
        "        cer = levenshtein(ground_truth, generated_text) / len(ground_truth)\n",
        "\n",
        "        return cer"
      ],
      "metadata": {
        "id": "EFX7Hqnk5gBC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model"
      ],
      "metadata": {
        "id": "7fzPTStJKh9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train:\")\n",
        "preprocessor_instance = Preprocessor(train)\n",
        "train_text = preprocessor_instance.col2text()\n",
        "\n",
        "print(\"\\n\\nTest:\")\n",
        "preprocessor_instance_2 = Preprocessor(test)\n",
        "test_text = preprocessor_instance.col2text()"
      ],
      "metadata": {
        "id": "yLz8aDgAKdV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089828b2-e217-4f91-d3a4-95d7c8f32194"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:\n",
            "Number of all chars = 3721028\n",
            "Frequency of each char:\n",
            "{'e': 2000, 'ﻧ': 1, 'ﺨ': 1, 'ض': 9616, ' ': 793194, 'ء': 785, 'غ': 4932, 'ژ': 2304, 'ی': 230377, 'ﺘ': 3, 'ع': 39849, 'ك': 15440, 'ﻣ': 6, 'ة': 709, 'ﺸ': 1, '\\\\': 4000, 'ﺑ': 3, 'د': 187919, 'ﺟ': 1, 'ﺣ': 1, 'ق': 29055, 'ذ': 5368, 'ﮔ': 1, 'N': 10781, 'ﻌ': 1, 'ى': 6243, '؟': 715, '_': 606, 'ﻦ': 2, 'ö': 3, 'ﻪ': 5, 'و': 166181, 'ﺮ': 3, 'ﻲ': 1, 'ﺎ': 5, 'ﭼ': 1, 'ت': 124002, 'ک': 56235, 'ۆ': 2, 'إ': 798, 'ä': 1, 'ز': 59292, 'ﺼ': 1, 'ﯿ': 1, 'ظ': 4588, 'ﮐ': 2, 'ش': 75228, 'أ': 2010, 'ﻳ': 1, 'ئ': 4215, '.': 23385, 'ل': 77632, 'ط': 13516, 'è': 1, 'خ': 38520, 'م': 172199, 'ﻨ': 2, 'ح': 29106, 'ﻘ': 1, 'پ': 17517, 'ﭘ': 1, 'ﻮ': 3, 'ć': 1, 'ﺒ': 3, 'ﺶ': 1, 'ﻠ': 1, 'ؤ': 330, 's': 2000, 'ﺖ': 2, 'ۀ': 26, 'ر': 247889, 'آ': 20239, 'ج': 32335, 'ﺳ': 2, 'ب': 125236, 'س': 88226, 'ه': 184905, 'ي': 42845, 'ھ': 5, 'ﺴ': 1, 'ـ': 348, 'á': 2, 'گ': 47922, 'ن': 212923, 'ف': 45279, 'ث': 5718, 'چ': 8654, '\\n': 4000, 'ا': 425977, 'ﻓ': 1, 'ە': 3, 'ص': 17781, 'ﺗ': 1}\n",
            "\n",
            "\n",
            "Test:\n",
            "Number of all chars = 390406\n",
            "Frequency of each char:\n",
            "{'ق': 3021, 'ئ': 448, 'ب': 13139, 'ذ': 573, 'س': 9460, 'N': 1020, 'ه': 19404, 'ي': 4403, 'e': 200, '.': 2403, 'ھ': 5, 'ـ': 32, 'ل': 7813, 'ط': 1473, 'خ': 4099, 'م': 18083, 'ى': 559, '؟': 71, '_': 3, 'ح': 2970, 'ض': 1055, 'گ': 5150, 'پ': 1911, 'ن': 22795, ' ': 83677, 'و': 17088, 'ف': 4589, 'غ': 507, 'ء': 89, 'ژ': 196, 'ی': 24062, 'ث': 544, 'ع': 4158, 'چ': 902, '\\n': 400, 'ت': 12688, 'ک': 6268, 'ك': 1497, 'إ': 128, 'ؤ': 62, 'ة': 95, 's': 200, 'ا': 44941, 'ز': 5990, 'ظ': 547, '\\\\': 400, 'ص': 1831, 'ر': 26264, 'آ': 2149, 'ج': 3341, 'ش': 7580, 'د': 19846, 'أ': 277}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make an instance from language model\n",
        "language_model = LanguageModel(text=train_text, CUDA=True)"
      ],
      "metadata": {
        "id": "2suRwh9kMn4X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the language model instance\n",
        "language_model.train_char_LSTM(epochs=5, n_seqs=256, n_steps=5, val_frac=0.1)"
      ],
      "metadata": {
        "id": "rkqEeF4sNpDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73802b49-e03a-4558-d099-2dcb091bb43e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5... Step: 10... Loss: 3.3181... Val Loss: 3.2081\n",
            "Epoch: 1/5... Step: 20... Loss: 3.1164... Val Loss: 3.1239\n",
            "Epoch: 1/5... Step: 30... Loss: 3.0531... Val Loss: 3.0367\n",
            "Epoch: 1/5... Step: 40... Loss: 2.9599... Val Loss: 2.9653\n",
            "Epoch: 1/5... Step: 50... Loss: 2.9336... Val Loss: 2.9009\n",
            "Epoch: 1/5... Step: 60... Loss: 2.8388... Val Loss: 2.8554\n",
            "Epoch: 1/5... Step: 70... Loss: 2.8325... Val Loss: 2.8178\n",
            "Epoch: 1/5... Step: 80... Loss: 2.6991... Val Loss: 2.7753\n",
            "Epoch: 1/5... Step: 90... Loss: 2.7368... Val Loss: 2.7445\n",
            "Epoch: 1/5... Step: 100... Loss: 2.7132... Val Loss: 2.7276\n",
            "Epoch: 1/5... Step: 110... Loss: 2.6649... Val Loss: 2.7373\n",
            "Epoch: 1/5... Step: 120... Loss: 2.6930... Val Loss: 2.7183\n",
            "Epoch: 1/5... Step: 130... Loss: 2.7260... Val Loss: 2.6550\n",
            "Epoch: 1/5... Step: 140... Loss: 2.6669... Val Loss: 2.6381\n",
            "Epoch: 1/5... Step: 150... Loss: 2.5874... Val Loss: 2.6214\n",
            "Epoch: 1/5... Step: 160... Loss: 2.5829... Val Loss: 2.6151\n",
            "Epoch: 1/5... Step: 170... Loss: 2.6164... Val Loss: 2.6742\n",
            "Epoch: 1/5... Step: 180... Loss: 2.5079... Val Loss: 2.5756\n",
            "Epoch: 1/5... Step: 190... Loss: 2.5424... Val Loss: 2.8069\n",
            "Epoch: 1/5... Step: 200... Loss: 2.5210... Val Loss: 2.5522\n",
            "Epoch: 1/5... Step: 210... Loss: 2.4370... Val Loss: 2.5297\n",
            "Epoch: 1/5... Step: 220... Loss: 2.4785... Val Loss: 2.5297\n",
            "Epoch: 1/5... Step: 230... Loss: 2.5214... Val Loss: 2.4949\n",
            "Epoch: 1/5... Step: 240... Loss: 2.5202... Val Loss: 2.4810\n",
            "Epoch: 1/5... Step: 250... Loss: 2.4950... Val Loss: 2.4927\n",
            "Epoch: 1/5... Step: 260... Loss: 2.3881... Val Loss: 2.4820\n",
            "Epoch: 1/5... Step: 270... Loss: 2.4656... Val Loss: 2.4571\n",
            "Epoch: 1/5... Step: 280... Loss: 2.4058... Val Loss: 2.4439\n",
            "Epoch: 1/5... Step: 290... Loss: 2.4740... Val Loss: 2.4612\n",
            "Epoch: 1/5... Step: 300... Loss: 2.3663... Val Loss: 2.4414\n",
            "Epoch: 1/5... Step: 310... Loss: 2.4195... Val Loss: 2.4448\n",
            "Epoch: 1/5... Step: 320... Loss: 2.4092... Val Loss: 2.4236\n",
            "Epoch: 1/5... Step: 330... Loss: 2.3207... Val Loss: 6.9138\n",
            "Epoch: 1/5... Step: 340... Loss: 2.4144... Val Loss: 2.3951\n",
            "Epoch: 1/5... Step: 350... Loss: 2.4057... Val Loss: 2.8202\n",
            "Epoch: 1/5... Step: 360... Loss: 2.3749... Val Loss: 2.3821\n",
            "Epoch: 1/5... Step: 370... Loss: 2.3446... Val Loss: 2.3657\n",
            "Epoch: 1/5... Step: 380... Loss: 2.3648... Val Loss: 2.3612\n",
            "Epoch: 1/5... Step: 390... Loss: 2.3511... Val Loss: 2.3691\n",
            "Epoch: 1/5... Step: 400... Loss: 2.3765... Val Loss: 2.3467\n",
            "Epoch: 1/5... Step: 410... Loss: 2.3249... Val Loss: 2.3555\n",
            "Epoch: 1/5... Step: 420... Loss: 2.2875... Val Loss: 2.3444\n",
            "Epoch: 1/5... Step: 430... Loss: 2.3018... Val Loss: 2.3320\n",
            "Epoch: 1/5... Step: 440... Loss: 2.3313... Val Loss: 2.3237\n",
            "Epoch: 1/5... Step: 450... Loss: 2.2514... Val Loss: 2.3201\n",
            "Epoch: 1/5... Step: 460... Loss: 2.2386... Val Loss: 2.3102\n",
            "Epoch: 1/5... Step: 470... Loss: 2.2184... Val Loss: 2.3127\n",
            "Epoch: 1/5... Step: 480... Loss: 2.2412... Val Loss: 2.3041\n",
            "Epoch: 1/5... Step: 490... Loss: 2.2570... Val Loss: 2.3002\n",
            "Epoch: 1/5... Step: 500... Loss: 2.1923... Val Loss: 2.2933\n",
            "Epoch: 1/5... Step: 510... Loss: 2.2441... Val Loss: 2.2841\n",
            "Epoch: 1/5... Step: 520... Loss: 2.2610... Val Loss: 2.2767\n",
            "Epoch: 1/5... Step: 530... Loss: 2.2043... Val Loss: 2.2745\n",
            "Epoch: 1/5... Step: 540... Loss: 2.2735... Val Loss: 2.2620\n",
            "Epoch: 1/5... Step: 550... Loss: 2.1753... Val Loss: 2.2728\n",
            "Epoch: 1/5... Step: 560... Loss: 2.2598... Val Loss: 2.2614\n",
            "Epoch: 1/5... Step: 570... Loss: 2.2157... Val Loss: 2.2605\n",
            "Epoch: 1/5... Step: 580... Loss: 2.1833... Val Loss: 2.2446\n",
            "Epoch: 1/5... Step: 590... Loss: 2.1547... Val Loss: 2.2428\n",
            "Epoch: 1/5... Step: 600... Loss: 2.1973... Val Loss: 2.2333\n",
            "Epoch: 1/5... Step: 610... Loss: 2.1552... Val Loss: 2.2342\n",
            "Epoch: 1/5... Step: 620... Loss: 2.1065... Val Loss: 2.2212\n",
            "Epoch: 1/5... Step: 630... Loss: 2.1759... Val Loss: 2.2200\n",
            "Epoch: 1/5... Step: 640... Loss: 2.1207... Val Loss: 2.2111\n",
            "Epoch: 1/5... Step: 650... Loss: 2.1696... Val Loss: 2.2082\n",
            "Epoch: 1/5... Step: 660... Loss: 2.1839... Val Loss: 2.2080\n",
            "Epoch: 1/5... Step: 670... Loss: 2.2316... Val Loss: 2.2071\n",
            "Epoch: 1/5... Step: 680... Loss: 2.1283... Val Loss: 2.2012\n",
            "Epoch: 1/5... Step: 690... Loss: 2.1108... Val Loss: 2.1937\n",
            "Epoch: 1/5... Step: 700... Loss: 2.0638... Val Loss: 2.1876\n",
            "Epoch: 1/5... Step: 710... Loss: 2.2527... Val Loss: 2.1901\n",
            "Epoch: 1/5... Step: 720... Loss: 2.2525... Val Loss: 2.1789\n",
            "Epoch: 1/5... Step: 730... Loss: 2.0649... Val Loss: 2.1720\n",
            "Epoch: 1/5... Step: 740... Loss: 2.0746... Val Loss: 2.1687\n",
            "Epoch: 1/5... Step: 750... Loss: 2.1664... Val Loss: 2.1747\n",
            "Epoch: 1/5... Step: 760... Loss: 2.1159... Val Loss: 2.1608\n",
            "Epoch: 1/5... Step: 770... Loss: 2.1909... Val Loss: 2.1663\n",
            "Epoch: 1/5... Step: 780... Loss: 2.1636... Val Loss: 2.1629\n",
            "Epoch: 1/5... Step: 790... Loss: 2.1639... Val Loss: 2.1631\n",
            "Epoch: 1/5... Step: 800... Loss: 2.0746... Val Loss: 2.1450\n",
            "Epoch: 1/5... Step: 810... Loss: 2.1218... Val Loss: 2.1448\n",
            "Epoch: 1/5... Step: 820... Loss: 2.1347... Val Loss: 2.1383\n",
            "Epoch: 1/5... Step: 830... Loss: 2.0618... Val Loss: 2.1418\n",
            "Epoch: 1/5... Step: 840... Loss: 2.1336... Val Loss: 2.1394\n",
            "Epoch: 1/5... Step: 850... Loss: 1.9808... Val Loss: 2.1299\n",
            "Epoch: 1/5... Step: 860... Loss: 1.9568... Val Loss: 2.1296\n",
            "Epoch: 1/5... Step: 870... Loss: 2.0231... Val Loss: 2.1294\n",
            "Epoch: 1/5... Step: 880... Loss: 2.0658... Val Loss: 2.1168\n",
            "Epoch: 1/5... Step: 890... Loss: 2.0323... Val Loss: 2.1154\n",
            "Epoch: 1/5... Step: 900... Loss: 2.0672... Val Loss: 2.1150\n",
            "Epoch: 1/5... Step: 910... Loss: 2.0859... Val Loss: 2.1195\n",
            "Epoch: 1/5... Step: 920... Loss: 2.0408... Val Loss: 2.1230\n",
            "Epoch: 1/5... Step: 930... Loss: 2.0746... Val Loss: 2.1032\n",
            "Epoch: 1/5... Step: 940... Loss: 2.0949... Val Loss: 2.1005\n",
            "Epoch: 1/5... Step: 950... Loss: 2.0378... Val Loss: 2.0953\n",
            "Epoch: 1/5... Step: 960... Loss: 2.0466... Val Loss: 2.0891\n",
            "Epoch: 1/5... Step: 970... Loss: 1.9781... Val Loss: 2.0912\n",
            "Epoch: 1/5... Step: 980... Loss: 2.0113... Val Loss: 2.0865\n",
            "Epoch: 1/5... Step: 990... Loss: 2.0574... Val Loss: 2.0849\n",
            "Epoch: 1/5... Step: 1000... Loss: 2.0328... Val Loss: 2.0760\n",
            "Epoch: 1/5... Step: 1010... Loss: 2.0268... Val Loss: 2.0815\n",
            "Epoch: 1/5... Step: 1020... Loss: 1.9813... Val Loss: 2.0773\n",
            "Epoch: 1/5... Step: 1030... Loss: 2.0278... Val Loss: 2.0738\n",
            "Epoch: 1/5... Step: 1040... Loss: 1.9910... Val Loss: 2.0664\n",
            "Epoch: 1/5... Step: 1050... Loss: 2.0282... Val Loss: 2.0708\n",
            "Epoch: 1/5... Step: 1060... Loss: 2.0126... Val Loss: 2.0668\n",
            "Epoch: 1/5... Step: 1070... Loss: 1.9591... Val Loss: 2.0644\n",
            "Epoch: 1/5... Step: 1080... Loss: 2.0171... Val Loss: 2.0553\n",
            "Epoch: 1/5... Step: 1090... Loss: 1.8480... Val Loss: 2.0546\n",
            "Epoch: 1/5... Step: 1100... Loss: 1.9004... Val Loss: 2.0497\n",
            "Epoch: 1/5... Step: 1110... Loss: 1.8973... Val Loss: 2.0427\n",
            "Epoch: 1/5... Step: 1120... Loss: 2.0331... Val Loss: 2.0467\n",
            "Epoch: 1/5... Step: 1130... Loss: 1.9300... Val Loss: 2.0542\n",
            "Epoch: 1/5... Step: 1140... Loss: 2.0453... Val Loss: 2.0419\n",
            "Epoch: 1/5... Step: 1150... Loss: 2.0231... Val Loss: 2.0428\n",
            "Epoch: 1/5... Step: 1160... Loss: 1.9257... Val Loss: 2.0456\n",
            "Epoch: 1/5... Step: 1170... Loss: 2.0526... Val Loss: 2.0550\n",
            "Epoch: 1/5... Step: 1180... Loss: 1.9747... Val Loss: 2.0375\n",
            "Epoch: 1/5... Step: 1190... Loss: 2.0995... Val Loss: 2.0315\n",
            "Epoch: 1/5... Step: 1200... Loss: 1.9246... Val Loss: 2.0189\n",
            "Epoch: 1/5... Step: 1210... Loss: 1.9370... Val Loss: 2.0218\n",
            "Epoch: 1/5... Step: 1220... Loss: 1.8717... Val Loss: 2.0249\n",
            "Epoch: 1/5... Step: 1230... Loss: 1.9633... Val Loss: 2.0289\n",
            "Epoch: 1/5... Step: 1240... Loss: 1.9889... Val Loss: 2.0265\n",
            "Epoch: 1/5... Step: 1250... Loss: 1.9829... Val Loss: 2.0167\n",
            "Epoch: 1/5... Step: 1260... Loss: 1.9754... Val Loss: 2.0098\n",
            "Epoch: 1/5... Step: 1270... Loss: 1.9355... Val Loss: 2.0161\n",
            "Epoch: 1/5... Step: 1280... Loss: 1.8972... Val Loss: 2.0150\n",
            "Epoch: 1/5... Step: 1290... Loss: 1.9723... Val Loss: 2.0190\n",
            "Epoch: 1/5... Step: 1300... Loss: 1.9305... Val Loss: 2.0118\n",
            "Epoch: 1/5... Step: 1310... Loss: 1.9916... Val Loss: 2.0142\n",
            "Epoch: 1/5... Step: 1320... Loss: 1.8085... Val Loss: 2.0039\n",
            "Epoch: 1/5... Step: 1330... Loss: 1.9827... Val Loss: 2.0105\n",
            "Epoch: 1/5... Step: 1340... Loss: 1.9586... Val Loss: 1.9926\n",
            "Epoch: 1/5... Step: 1350... Loss: 1.9466... Val Loss: 1.9919\n",
            "Epoch: 1/5... Step: 1360... Loss: 1.8757... Val Loss: 1.9894\n",
            "Epoch: 1/5... Step: 1370... Loss: 1.9302... Val Loss: 2.5040\n",
            "Epoch: 1/5... Step: 1380... Loss: 1.9638... Val Loss: 1.9977\n",
            "Epoch: 1/5... Step: 1390... Loss: 1.8606... Val Loss: 1.9900\n",
            "Epoch: 1/5... Step: 1400... Loss: 1.8665... Val Loss: 1.9868\n",
            "Epoch: 1/5... Step: 1410... Loss: 2.0186... Val Loss: 1.9805\n",
            "Epoch: 1/5... Step: 1420... Loss: 1.9682... Val Loss: 1.9808\n",
            "Epoch: 1/5... Step: 1430... Loss: 1.8635... Val Loss: 1.9805\n",
            "Epoch: 1/5... Step: 1440... Loss: 1.9834... Val Loss: 1.9836\n",
            "Epoch: 1/5... Step: 1450... Loss: 1.9376... Val Loss: 1.9856\n",
            "Epoch: 1/5... Step: 1460... Loss: 1.8462... Val Loss: 1.9812\n",
            "Epoch: 1/5... Step: 1470... Loss: 1.8808... Val Loss: 1.9765\n",
            "Epoch: 1/5... Step: 1480... Loss: 1.8643... Val Loss: 1.9695\n",
            "Epoch: 1/5... Step: 1490... Loss: 1.8513... Val Loss: 1.9781\n",
            "Epoch: 1/5... Step: 1500... Loss: 1.8180... Val Loss: 1.9693\n",
            "Epoch: 1/5... Step: 1510... Loss: 1.8475... Val Loss: 1.9710\n",
            "Epoch: 1/5... Step: 1520... Loss: 1.8177... Val Loss: 1.9605\n",
            "Epoch: 1/5... Step: 1530... Loss: 1.8019... Val Loss: 1.9621\n",
            "Epoch: 1/5... Step: 1540... Loss: 1.8433... Val Loss: 1.9517\n",
            "Epoch: 1/5... Step: 1550... Loss: 1.7827... Val Loss: 1.9565\n",
            "Epoch: 1/5... Step: 1560... Loss: 1.8396... Val Loss: 1.9524\n",
            "Epoch: 1/5... Step: 1570... Loss: 1.8341... Val Loss: 1.9545\n",
            "Epoch: 1/5... Step: 1580... Loss: 1.8870... Val Loss: 1.9507\n",
            "Epoch: 1/5... Step: 1590... Loss: 1.8956... Val Loss: 1.9525\n",
            "Epoch: 1/5... Step: 1600... Loss: 1.8222... Val Loss: 1.9571\n",
            "Epoch: 1/5... Step: 1610... Loss: 1.8734... Val Loss: 1.9461\n",
            "Epoch: 1/5... Step: 1620... Loss: 1.8863... Val Loss: 1.9469\n",
            "Epoch: 1/5... Step: 1630... Loss: 1.8832... Val Loss: 1.9402\n",
            "Epoch: 1/5... Step: 1640... Loss: 1.8076... Val Loss: 1.9443\n",
            "Epoch: 1/5... Step: 1650... Loss: 1.8628... Val Loss: 1.9444\n",
            "Epoch: 1/5... Step: 1660... Loss: 1.8915... Val Loss: 1.9577\n",
            "Epoch: 1/5... Step: 1670... Loss: 1.8921... Val Loss: 1.9394\n",
            "Epoch: 1/5... Step: 1680... Loss: 1.8785... Val Loss: 1.9458\n",
            "Epoch: 1/5... Step: 1690... Loss: 1.8976... Val Loss: 1.9377\n",
            "Epoch: 1/5... Step: 1700... Loss: 1.8609... Val Loss: 1.9337\n",
            "Epoch: 1/5... Step: 1710... Loss: 1.9682... Val Loss: 1.9291\n",
            "Epoch: 1/5... Step: 1720... Loss: 1.7835... Val Loss: 1.9295\n",
            "Epoch: 1/5... Step: 1730... Loss: 1.9140... Val Loss: 1.9302\n",
            "Epoch: 1/5... Step: 1740... Loss: 1.8573... Val Loss: 1.9284\n",
            "Epoch: 1/5... Step: 1750... Loss: 1.8837... Val Loss: 1.9346\n",
            "Epoch: 1/5... Step: 1760... Loss: 1.8681... Val Loss: 1.9266\n",
            "Epoch: 1/5... Step: 1770... Loss: 1.9302... Val Loss: 1.9271\n",
            "Epoch: 1/5... Step: 1780... Loss: 1.8019... Val Loss: 1.9195\n",
            "Epoch: 1/5... Step: 1790... Loss: 1.8270... Val Loss: 1.9248\n",
            "Epoch: 1/5... Step: 1800... Loss: 1.8515... Val Loss: 1.9238\n",
            "Epoch: 1/5... Step: 1810... Loss: 1.9300... Val Loss: 1.9288\n",
            "Epoch: 1/5... Step: 1820... Loss: 1.8272... Val Loss: 1.9182\n",
            "Epoch: 1/5... Step: 1830... Loss: 1.8508... Val Loss: 1.9122\n",
            "Epoch: 1/5... Step: 1840... Loss: 1.8552... Val Loss: 1.9078\n",
            "Epoch: 1/5... Step: 1850... Loss: 1.7966... Val Loss: 1.9075\n",
            "Epoch: 1/5... Step: 1860... Loss: 1.8326... Val Loss: 1.9049\n",
            "Epoch: 1/5... Step: 1870... Loss: 1.7377... Val Loss: 1.9065\n",
            "Epoch: 1/5... Step: 1880... Loss: 1.7998... Val Loss: 1.9050\n",
            "Epoch: 1/5... Step: 1890... Loss: 1.8718... Val Loss: 1.9030\n",
            "Epoch: 1/5... Step: 1900... Loss: 1.8763... Val Loss: 1.9012\n",
            "Epoch: 1/5... Step: 1910... Loss: 1.8771... Val Loss: 1.8986\n",
            "Epoch: 1/5... Step: 1920... Loss: 1.8221... Val Loss: 1.9005\n",
            "Epoch: 1/5... Step: 1930... Loss: 1.8223... Val Loss: 1.8945\n",
            "Epoch: 1/5... Step: 1940... Loss: 1.9005... Val Loss: 1.8975\n",
            "Epoch: 1/5... Step: 1950... Loss: 1.8954... Val Loss: 1.8972\n",
            "Epoch: 1/5... Step: 1960... Loss: 1.8871... Val Loss: 1.8946\n",
            "Epoch: 1/5... Step: 1970... Loss: 1.7511... Val Loss: 1.8979\n",
            "Epoch: 1/5... Step: 1980... Loss: 1.8078... Val Loss: 1.8927\n",
            "Epoch: 1/5... Step: 1990... Loss: 1.6974... Val Loss: 1.9025\n",
            "Epoch: 1/5... Step: 2000... Loss: 1.7645... Val Loss: 1.8932\n",
            "Epoch: 1/5... Step: 2010... Loss: 1.8370... Val Loss: 1.9003\n",
            "Epoch: 1/5... Step: 2020... Loss: 1.9094... Val Loss: 1.8982\n",
            "Epoch: 1/5... Step: 2030... Loss: 1.8374... Val Loss: 1.8929\n",
            "Epoch: 1/5... Step: 2040... Loss: 1.8116... Val Loss: 1.8900\n",
            "Epoch: 1/5... Step: 2050... Loss: 1.8427... Val Loss: 1.8868\n",
            "Epoch: 1/5... Step: 2060... Loss: 1.8680... Val Loss: 1.8830\n",
            "Epoch: 1/5... Step: 2070... Loss: 1.8075... Val Loss: 1.9841\n",
            "Epoch: 1/5... Step: 2080... Loss: 1.7844... Val Loss: 1.8796\n",
            "Epoch: 1/5... Step: 2090... Loss: 1.7871... Val Loss: 1.9355\n",
            "Epoch: 1/5... Step: 2100... Loss: 1.8146... Val Loss: 1.8755\n",
            "Epoch: 1/5... Step: 2110... Loss: 1.7364... Val Loss: 1.8749\n",
            "Epoch: 1/5... Step: 2120... Loss: 1.6999... Val Loss: 1.8710\n",
            "Epoch: 1/5... Step: 2130... Loss: 1.8162... Val Loss: 1.8738\n",
            "Epoch: 1/5... Step: 2140... Loss: 1.6814... Val Loss: 1.8714\n",
            "Epoch: 1/5... Step: 2150... Loss: 1.7864... Val Loss: 1.8724\n",
            "Epoch: 1/5... Step: 2160... Loss: 1.8285... Val Loss: 1.8683\n",
            "Epoch: 1/5... Step: 2170... Loss: 1.8161... Val Loss: 1.8677\n",
            "Epoch: 1/5... Step: 2180... Loss: 1.7831... Val Loss: 1.8715\n",
            "Epoch: 1/5... Step: 2190... Loss: 1.6961... Val Loss: 1.8669\n",
            "Epoch: 1/5... Step: 2200... Loss: 1.7928... Val Loss: 1.8683\n",
            "Epoch: 1/5... Step: 2210... Loss: 1.7944... Val Loss: 1.8653\n",
            "Epoch: 1/5... Step: 2220... Loss: 1.7670... Val Loss: 1.8602\n",
            "Epoch: 1/5... Step: 2230... Loss: 1.8246... Val Loss: 1.8655\n",
            "Epoch: 1/5... Step: 2240... Loss: 1.7979... Val Loss: 1.8681\n",
            "Epoch: 1/5... Step: 2250... Loss: 1.8349... Val Loss: 1.8687\n",
            "Epoch: 1/5... Step: 2260... Loss: 1.7320... Val Loss: 1.8620\n",
            "Epoch: 1/5... Step: 2270... Loss: 1.7835... Val Loss: 1.8580\n",
            "Epoch: 1/5... Step: 2280... Loss: 1.7286... Val Loss: 1.8528\n",
            "Epoch: 1/5... Step: 2290... Loss: 1.8260... Val Loss: 1.8540\n",
            "Epoch: 1/5... Step: 2300... Loss: 1.7968... Val Loss: 1.8513\n",
            "Epoch: 1/5... Step: 2310... Loss: 1.7909... Val Loss: 1.8513\n",
            "Epoch: 1/5... Step: 2320... Loss: 1.7554... Val Loss: 1.8511\n",
            "Epoch: 1/5... Step: 2330... Loss: 1.7837... Val Loss: 1.8516\n",
            "Epoch: 1/5... Step: 2340... Loss: 1.8874... Val Loss: 1.8521\n",
            "Epoch: 1/5... Step: 2350... Loss: 1.8185... Val Loss: 1.8530\n",
            "Epoch: 1/5... Step: 2360... Loss: 1.7953... Val Loss: 1.8480\n",
            "Epoch: 1/5... Step: 2370... Loss: 1.7869... Val Loss: 1.8415\n",
            "Epoch: 1/5... Step: 2380... Loss: 1.8125... Val Loss: 1.8436\n",
            "Epoch: 1/5... Step: 2390... Loss: 1.7617... Val Loss: 1.8393\n",
            "Epoch: 1/5... Step: 2400... Loss: 1.8141... Val Loss: 1.8401\n",
            "Epoch: 1/5... Step: 2410... Loss: 1.8461... Val Loss: 1.8403\n",
            "Epoch: 1/5... Step: 2420... Loss: 1.7354... Val Loss: 1.8390\n",
            "Epoch: 1/5... Step: 2430... Loss: 1.8599... Val Loss: 1.8369\n",
            "Epoch: 1/5... Step: 2440... Loss: 1.7855... Val Loss: 1.8380\n",
            "Epoch: 1/5... Step: 2450... Loss: 1.7540... Val Loss: 1.8354\n",
            "Epoch: 1/5... Step: 2460... Loss: 1.8533... Val Loss: 1.8385\n",
            "Epoch: 1/5... Step: 2470... Loss: 1.7623... Val Loss: 1.8386\n",
            "Epoch: 1/5... Step: 2480... Loss: 1.8242... Val Loss: 1.8316\n",
            "Epoch: 1/5... Step: 2490... Loss: 1.7203... Val Loss: 1.8335\n",
            "Epoch: 1/5... Step: 2500... Loss: 1.7530... Val Loss: 1.8348\n",
            "Epoch: 1/5... Step: 2510... Loss: 1.7044... Val Loss: 1.8330\n",
            "Epoch: 1/5... Step: 2520... Loss: 1.7111... Val Loss: 1.8400\n",
            "Epoch: 1/5... Step: 2530... Loss: 1.7568... Val Loss: 1.8335\n",
            "Epoch: 1/5... Step: 2540... Loss: 1.6747... Val Loss: 1.8404\n",
            "Epoch: 1/5... Step: 2550... Loss: 1.7910... Val Loss: 1.8332\n",
            "Epoch: 1/5... Step: 2560... Loss: 1.7019... Val Loss: 1.8308\n",
            "Epoch: 1/5... Step: 2570... Loss: 1.7405... Val Loss: 1.8312\n",
            "Epoch: 1/5... Step: 2580... Loss: 1.8026... Val Loss: 1.8275\n",
            "Epoch: 1/5... Step: 2590... Loss: 1.7544... Val Loss: 1.8260\n",
            "Epoch: 1/5... Step: 2600... Loss: 1.7978... Val Loss: 1.8331\n",
            "Epoch: 1/5... Step: 2610... Loss: 1.8137... Val Loss: 1.8320\n",
            "Epoch: 2/5... Step: 2620... Loss: 1.8554... Val Loss: 1.8945\n",
            "Epoch: 2/5... Step: 2630... Loss: 1.8711... Val Loss: 1.8597\n",
            "Epoch: 2/5... Step: 2640... Loss: 1.8397... Val Loss: 1.8362\n",
            "Epoch: 2/5... Step: 2650... Loss: 1.8749... Val Loss: 1.8324\n",
            "Epoch: 2/5... Step: 2660... Loss: 1.7566... Val Loss: 1.8237\n",
            "Epoch: 2/5... Step: 2670... Loss: 1.7596... Val Loss: 1.8200\n",
            "Epoch: 2/5... Step: 2680... Loss: 1.7265... Val Loss: 1.8191\n",
            "Epoch: 2/5... Step: 2690... Loss: 1.7202... Val Loss: 1.8202\n",
            "Epoch: 2/5... Step: 2700... Loss: 1.7420... Val Loss: 1.8174\n",
            "Epoch: 2/5... Step: 2710... Loss: 1.7299... Val Loss: 1.8185\n",
            "Epoch: 2/5... Step: 2720... Loss: 1.7380... Val Loss: 1.8188\n",
            "Epoch: 2/5... Step: 2730... Loss: 1.7744... Val Loss: 1.8167\n",
            "Epoch: 2/5... Step: 2740... Loss: 1.7528... Val Loss: 1.8149\n",
            "Epoch: 2/5... Step: 2750... Loss: 1.7828... Val Loss: 1.8146\n",
            "Epoch: 2/5... Step: 2760... Loss: 1.8366... Val Loss: 1.8144\n",
            "Epoch: 2/5... Step: 2770... Loss: 1.7746... Val Loss: 1.8136\n",
            "Epoch: 2/5... Step: 2780... Loss: 1.7743... Val Loss: 1.8164\n",
            "Epoch: 2/5... Step: 2790... Loss: 1.6640... Val Loss: 1.8095\n",
            "Epoch: 2/5... Step: 2800... Loss: 1.7026... Val Loss: 1.8128\n",
            "Epoch: 2/5... Step: 2810... Loss: 1.6836... Val Loss: 1.8047\n",
            "Epoch: 2/5... Step: 2820... Loss: 1.7555... Val Loss: 1.8040\n",
            "Epoch: 2/5... Step: 2830... Loss: 1.7164... Val Loss: 1.8090\n",
            "Epoch: 2/5... Step: 2840... Loss: 1.6647... Val Loss: 1.8077\n",
            "Epoch: 2/5... Step: 2850... Loss: 1.7632... Val Loss: 1.8069\n",
            "Epoch: 2/5... Step: 2860... Loss: 1.7764... Val Loss: 1.8078\n",
            "Epoch: 2/5... Step: 2870... Loss: 1.6855... Val Loss: 1.8087\n",
            "Epoch: 2/5... Step: 2880... Loss: 1.6654... Val Loss: 1.8061\n",
            "Epoch: 2/5... Step: 2890... Loss: 1.7121... Val Loss: 1.8034\n",
            "Epoch: 2/5... Step: 2900... Loss: 1.6796... Val Loss: 1.8015\n",
            "Epoch: 2/5... Step: 2910... Loss: 1.6716... Val Loss: 1.8032\n",
            "Epoch: 2/5... Step: 2920... Loss: 1.7409... Val Loss: 1.7994\n",
            "Epoch: 2/5... Step: 2930... Loss: 1.7177... Val Loss: 1.8021\n",
            "Epoch: 2/5... Step: 2940... Loss: 1.6953... Val Loss: 1.8000\n",
            "Epoch: 2/5... Step: 2950... Loss: 1.7483... Val Loss: 1.8046\n",
            "Epoch: 2/5... Step: 2960... Loss: 1.6893... Val Loss: 1.8005\n",
            "Epoch: 2/5... Step: 2970... Loss: 1.6520... Val Loss: 1.7980\n",
            "Epoch: 2/5... Step: 2980... Loss: 1.7653... Val Loss: 1.7944\n",
            "Epoch: 2/5... Step: 2990... Loss: 1.7837... Val Loss: 1.7974\n",
            "Epoch: 2/5... Step: 3000... Loss: 1.6586... Val Loss: 1.7942\n",
            "Epoch: 2/5... Step: 3010... Loss: 1.6893... Val Loss: 1.7931\n",
            "Epoch: 2/5... Step: 3020... Loss: 1.6655... Val Loss: 1.7920\n",
            "Epoch: 2/5... Step: 3030... Loss: 1.6792... Val Loss: 1.7914\n",
            "Epoch: 2/5... Step: 3040... Loss: 1.6493... Val Loss: 1.7880\n",
            "Epoch: 2/5... Step: 3050... Loss: 1.6612... Val Loss: 1.7909\n",
            "Epoch: 2/5... Step: 3060... Loss: 1.7307... Val Loss: 1.7885\n",
            "Epoch: 2/5... Step: 3070... Loss: 1.6890... Val Loss: 1.7933\n",
            "Epoch: 2/5... Step: 3080... Loss: 1.7260... Val Loss: 1.7901\n",
            "Epoch: 2/5... Step: 3090... Loss: 1.6100... Val Loss: 1.7936\n",
            "Epoch: 2/5... Step: 3100... Loss: 1.7463... Val Loss: 1.7899\n",
            "Epoch: 2/5... Step: 3110... Loss: 1.7464... Val Loss: 1.7932\n",
            "Epoch: 2/5... Step: 3120... Loss: 1.6452... Val Loss: 1.7932\n",
            "Epoch: 2/5... Step: 3130... Loss: 1.7245... Val Loss: 1.7886\n",
            "Epoch: 2/5... Step: 3140... Loss: 1.6550... Val Loss: 1.7885\n",
            "Epoch: 2/5... Step: 3150... Loss: 1.7297... Val Loss: 1.7868\n",
            "Epoch: 2/5... Step: 3160... Loss: 1.7067... Val Loss: 1.7865\n",
            "Epoch: 2/5... Step: 3170... Loss: 1.7850... Val Loss: 1.7814\n",
            "Epoch: 2/5... Step: 3180... Loss: 1.7538... Val Loss: 1.7856\n",
            "Epoch: 2/5... Step: 3190... Loss: 1.7363... Val Loss: 1.7895\n",
            "Epoch: 2/5... Step: 3200... Loss: 1.7142... Val Loss: 1.7883\n",
            "Epoch: 2/5... Step: 3210... Loss: 1.6923... Val Loss: 1.7899\n",
            "Epoch: 2/5... Step: 3220... Loss: 1.6710... Val Loss: 1.7830\n",
            "Epoch: 2/5... Step: 3230... Loss: 1.6838... Val Loss: 1.7864\n",
            "Epoch: 2/5... Step: 3240... Loss: 1.5999... Val Loss: 1.7868\n",
            "Epoch: 2/5... Step: 3250... Loss: 1.7062... Val Loss: 1.7864\n",
            "Epoch: 2/5... Step: 3260... Loss: 1.6651... Val Loss: 1.7801\n",
            "Epoch: 2/5... Step: 3270... Loss: 1.6505... Val Loss: 1.7819\n",
            "Epoch: 2/5... Step: 3280... Loss: 1.7330... Val Loss: 1.7769\n",
            "Epoch: 2/5... Step: 3290... Loss: 1.6947... Val Loss: 1.7766\n",
            "Epoch: 2/5... Step: 3300... Loss: 1.6712... Val Loss: 1.7749\n",
            "Epoch: 2/5... Step: 3310... Loss: 1.7574... Val Loss: 1.7802\n",
            "Epoch: 2/5... Step: 3320... Loss: 1.6806... Val Loss: 1.7782\n",
            "Epoch: 2/5... Step: 3330... Loss: 1.6789... Val Loss: 1.7844\n",
            "Epoch: 2/5... Step: 3340... Loss: 1.6953... Val Loss: 1.7857\n",
            "Epoch: 2/5... Step: 3350... Loss: 1.6565... Val Loss: 1.7808\n",
            "Epoch: 2/5... Step: 3360... Loss: 1.6269... Val Loss: 1.7779\n",
            "Epoch: 2/5... Step: 3370... Loss: 1.6898... Val Loss: 1.7746\n",
            "Epoch: 2/5... Step: 3380... Loss: 1.6797... Val Loss: 1.7719\n",
            "Epoch: 2/5... Step: 3390... Loss: 1.7811... Val Loss: 1.7724\n",
            "Epoch: 2/5... Step: 3400... Loss: 1.6904... Val Loss: 1.7689\n",
            "Epoch: 2/5... Step: 3410... Loss: 1.7273... Val Loss: 1.7715\n",
            "Epoch: 2/5... Step: 3420... Loss: 1.6234... Val Loss: 1.7663\n",
            "Epoch: 2/5... Step: 3430... Loss: 1.7584... Val Loss: 1.7699\n",
            "Epoch: 2/5... Step: 3440... Loss: 1.7108... Val Loss: 1.7731\n",
            "Epoch: 2/5... Step: 3450... Loss: 1.6981... Val Loss: 1.7777\n",
            "Epoch: 2/5... Step: 3460... Loss: 1.6940... Val Loss: 1.7675\n",
            "Epoch: 2/5... Step: 3470... Loss: 1.6837... Val Loss: 1.7739\n",
            "Epoch: 2/5... Step: 3480... Loss: 1.6526... Val Loss: 1.7682\n",
            "Epoch: 2/5... Step: 3490... Loss: 1.6824... Val Loss: 1.7689\n",
            "Epoch: 2/5... Step: 3500... Loss: 1.7411... Val Loss: 1.7650\n",
            "Epoch: 2/5... Step: 3510... Loss: 1.6652... Val Loss: 1.7760\n",
            "Epoch: 2/5... Step: 3520... Loss: 1.6633... Val Loss: 1.7711\n",
            "Epoch: 2/5... Step: 3530... Loss: 1.6976... Val Loss: 1.7699\n",
            "Epoch: 2/5... Step: 3540... Loss: 1.7618... Val Loss: 1.7671\n",
            "Epoch: 2/5... Step: 3550... Loss: 1.6697... Val Loss: 1.7654\n",
            "Epoch: 2/5... Step: 3560... Loss: 1.6725... Val Loss: 1.7631\n",
            "Epoch: 2/5... Step: 3570... Loss: 1.6860... Val Loss: 1.7613\n",
            "Epoch: 2/5... Step: 3580... Loss: 1.5759... Val Loss: 1.7636\n",
            "Epoch: 2/5... Step: 3590... Loss: 1.6746... Val Loss: 1.7606\n",
            "Epoch: 2/5... Step: 3600... Loss: 1.7123... Val Loss: 1.7617\n",
            "Epoch: 2/5... Step: 3610... Loss: 1.6456... Val Loss: 1.7574\n",
            "Epoch: 2/5... Step: 3620... Loss: 1.6491... Val Loss: 1.7611\n",
            "Epoch: 2/5... Step: 3630... Loss: 1.7187... Val Loss: 1.7627\n",
            "Epoch: 2/5... Step: 3640... Loss: 1.7367... Val Loss: 1.7589\n",
            "Epoch: 2/5... Step: 3650... Loss: 1.6839... Val Loss: 1.7606\n",
            "Epoch: 2/5... Step: 3660... Loss: 1.6956... Val Loss: 1.7654\n",
            "Epoch: 2/5... Step: 3670... Loss: 1.5743... Val Loss: 1.7614\n",
            "Epoch: 2/5... Step: 3680... Loss: 1.7387... Val Loss: 1.7575\n",
            "Epoch: 2/5... Step: 3690... Loss: 1.6871... Val Loss: 1.7616\n",
            "Epoch: 2/5... Step: 3700... Loss: 1.5732... Val Loss: 1.7579\n",
            "Epoch: 2/5... Step: 3710... Loss: 1.7103... Val Loss: 1.7553\n",
            "Epoch: 2/5... Step: 3720... Loss: 1.7209... Val Loss: 1.7604\n",
            "Epoch: 2/5... Step: 3730... Loss: 1.7038... Val Loss: 1.7547\n",
            "Epoch: 2/5... Step: 3740... Loss: 1.6662... Val Loss: 1.7628\n",
            "Epoch: 2/5... Step: 3750... Loss: 1.6407... Val Loss: 1.7644\n",
            "Epoch: 2/5... Step: 3760... Loss: 1.6919... Val Loss: 1.7542\n",
            "Epoch: 2/5... Step: 3770... Loss: 1.7043... Val Loss: 1.7515\n",
            "Epoch: 2/5... Step: 3780... Loss: 1.7601... Val Loss: 1.7515\n",
            "Epoch: 2/5... Step: 3790... Loss: 1.7069... Val Loss: 1.7491\n",
            "Epoch: 2/5... Step: 3800... Loss: 1.5827... Val Loss: 1.7513\n",
            "Epoch: 2/5... Step: 3810... Loss: 1.6972... Val Loss: 1.7513\n",
            "Epoch: 2/5... Step: 3820... Loss: 1.7340... Val Loss: 1.7522\n",
            "Epoch: 2/5... Step: 3830... Loss: 1.6196... Val Loss: 1.7484\n",
            "Epoch: 2/5... Step: 3840... Loss: 1.6645... Val Loss: 1.7475\n",
            "Epoch: 2/5... Step: 3850... Loss: 1.6348... Val Loss: 1.7505\n",
            "Epoch: 2/5... Step: 3860... Loss: 1.6783... Val Loss: 1.7499\n",
            "Epoch: 2/5... Step: 3870... Loss: 1.6941... Val Loss: 1.7522\n",
            "Epoch: 2/5... Step: 3880... Loss: 1.6236... Val Loss: 1.7572\n",
            "Epoch: 2/5... Step: 3890... Loss: 1.6073... Val Loss: 1.7485\n",
            "Epoch: 2/5... Step: 3900... Loss: 1.7065... Val Loss: 1.7504\n",
            "Epoch: 2/5... Step: 3910... Loss: 1.6110... Val Loss: 1.7490\n",
            "Epoch: 2/5... Step: 3920... Loss: 1.6666... Val Loss: 1.7567\n",
            "Epoch: 2/5... Step: 3930... Loss: 1.7582... Val Loss: 1.7615\n",
            "Epoch: 2/5... Step: 3940... Loss: 1.6757... Val Loss: 1.7536\n",
            "Epoch: 2/5... Step: 3950... Loss: 1.6771... Val Loss: 1.7459\n",
            "Epoch: 2/5... Step: 3960... Loss: 1.7110... Val Loss: 1.7446\n",
            "Epoch: 2/5... Step: 3970... Loss: 1.7193... Val Loss: 1.7516\n",
            "Epoch: 2/5... Step: 3980... Loss: 1.6227... Val Loss: 1.7456\n",
            "Epoch: 2/5... Step: 3990... Loss: 1.6530... Val Loss: 1.7436\n",
            "Epoch: 2/5... Step: 4000... Loss: 1.6599... Val Loss: 1.7467\n",
            "Epoch: 2/5... Step: 4010... Loss: 1.5742... Val Loss: 1.7484\n",
            "Epoch: 2/5... Step: 4020... Loss: 1.7437... Val Loss: 1.7437\n",
            "Epoch: 2/5... Step: 4030... Loss: 1.6874... Val Loss: 1.7386\n",
            "Epoch: 2/5... Step: 4040... Loss: 1.6421... Val Loss: 1.7442\n",
            "Epoch: 2/5... Step: 4050... Loss: 1.7294... Val Loss: 1.7445\n",
            "Epoch: 2/5... Step: 4060... Loss: 1.6273... Val Loss: 1.7411\n",
            "Epoch: 2/5... Step: 4070... Loss: 1.5695... Val Loss: 1.7425\n",
            "Epoch: 2/5... Step: 4080... Loss: 1.6534... Val Loss: 1.7395\n",
            "Epoch: 2/5... Step: 4090... Loss: 1.6258... Val Loss: 1.7421\n",
            "Epoch: 2/5... Step: 4100... Loss: 1.6465... Val Loss: 1.7394\n",
            "Epoch: 2/5... Step: 4110... Loss: 1.6689... Val Loss: 1.7386\n",
            "Epoch: 2/5... Step: 4120... Loss: 1.6406... Val Loss: 1.7445\n",
            "Epoch: 2/5... Step: 4130... Loss: 1.6342... Val Loss: 1.7398\n",
            "Epoch: 2/5... Step: 4140... Loss: 1.6130... Val Loss: 1.7462\n",
            "Epoch: 2/5... Step: 4150... Loss: 1.5916... Val Loss: 1.7419\n",
            "Epoch: 2/5... Step: 4160... Loss: 1.6491... Val Loss: 1.7379\n",
            "Epoch: 2/5... Step: 4170... Loss: 1.6063... Val Loss: 1.7386\n",
            "Epoch: 2/5... Step: 4180... Loss: 1.7260... Val Loss: 1.7411\n",
            "Epoch: 2/5... Step: 4190... Loss: 1.6087... Val Loss: 1.7356\n",
            "Epoch: 2/5... Step: 4200... Loss: 1.5783... Val Loss: 1.7346\n",
            "Epoch: 2/5... Step: 4210... Loss: 1.5957... Val Loss: 1.7357\n",
            "Epoch: 2/5... Step: 4220... Loss: 1.6528... Val Loss: 1.7384\n",
            "Epoch: 2/5... Step: 4230... Loss: 1.6998... Val Loss: 1.7372\n",
            "Epoch: 2/5... Step: 4240... Loss: 1.5128... Val Loss: 1.7318\n",
            "Epoch: 2/5... Step: 4250... Loss: 1.6400... Val Loss: 1.7315\n",
            "Epoch: 2/5... Step: 4260... Loss: 1.5204... Val Loss: 1.7349\n",
            "Epoch: 2/5... Step: 4270... Loss: 1.5972... Val Loss: 1.7342\n",
            "Epoch: 2/5... Step: 4280... Loss: 1.6619... Val Loss: 1.7347\n",
            "Epoch: 2/5... Step: 4290... Loss: 1.8059... Val Loss: 1.7319\n",
            "Epoch: 2/5... Step: 4300... Loss: 1.6833... Val Loss: 1.7319\n",
            "Epoch: 2/5... Step: 4310... Loss: 1.6377... Val Loss: 1.7371\n",
            "Epoch: 2/5... Step: 4320... Loss: 1.7125... Val Loss: 1.7361\n",
            "Epoch: 2/5... Step: 4330... Loss: 1.7017... Val Loss: 1.7330\n",
            "Epoch: 2/5... Step: 4340... Loss: 1.6140... Val Loss: 1.7382\n",
            "Epoch: 2/5... Step: 4350... Loss: 1.6354... Val Loss: 1.7307\n",
            "Epoch: 2/5... Step: 4360... Loss: 1.6638... Val Loss: 1.7344\n",
            "Epoch: 2/5... Step: 4370... Loss: 1.6298... Val Loss: 1.7308\n",
            "Epoch: 2/5... Step: 4380... Loss: 1.6974... Val Loss: 1.7265\n",
            "Epoch: 2/5... Step: 4390... Loss: 1.6222... Val Loss: 1.7330\n",
            "Epoch: 2/5... Step: 4400... Loss: 1.6286... Val Loss: 1.7261\n",
            "Epoch: 2/5... Step: 4410... Loss: 1.7100... Val Loss: 1.7291\n",
            "Epoch: 2/5... Step: 4420... Loss: 1.6075... Val Loss: 1.7287\n",
            "Epoch: 2/5... Step: 4430... Loss: 1.5263... Val Loss: 1.7309\n",
            "Epoch: 2/5... Step: 4440... Loss: 1.5989... Val Loss: 1.7275\n",
            "Epoch: 2/5... Step: 4450... Loss: 1.6490... Val Loss: 1.7294\n",
            "Epoch: 2/5... Step: 4460... Loss: 1.6645... Val Loss: 1.7266\n",
            "Epoch: 2/5... Step: 4470... Loss: 1.6512... Val Loss: 1.7272\n",
            "Epoch: 2/5... Step: 4480... Loss: 1.6531... Val Loss: 1.7293\n",
            "Epoch: 2/5... Step: 4490... Loss: 1.6158... Val Loss: 1.7253\n",
            "Epoch: 2/5... Step: 4500... Loss: 1.5351... Val Loss: 1.7253\n",
            "Epoch: 2/5... Step: 4510... Loss: 1.6116... Val Loss: 1.7324\n",
            "Epoch: 2/5... Step: 4520... Loss: 1.6919... Val Loss: 1.7259\n",
            "Epoch: 2/5... Step: 4530... Loss: 1.6982... Val Loss: 1.7221\n",
            "Epoch: 2/5... Step: 4540... Loss: 1.5530... Val Loss: 1.7257\n",
            "Epoch: 2/5... Step: 4550... Loss: 1.5992... Val Loss: 1.7264\n",
            "Epoch: 2/5... Step: 4560... Loss: 1.7616... Val Loss: 1.7236\n",
            "Epoch: 2/5... Step: 4570... Loss: 1.7055... Val Loss: 1.7228\n",
            "Epoch: 2/5... Step: 4580... Loss: 1.6450... Val Loss: 1.7272\n",
            "Epoch: 2/5... Step: 4590... Loss: 1.6848... Val Loss: 1.7225\n",
            "Epoch: 2/5... Step: 4600... Loss: 1.5920... Val Loss: 1.7190\n",
            "Epoch: 2/5... Step: 4610... Loss: 1.5760... Val Loss: 1.7236\n",
            "Epoch: 2/5... Step: 4620... Loss: 1.6470... Val Loss: 1.7201\n",
            "Epoch: 2/5... Step: 4630... Loss: 1.6267... Val Loss: 1.7247\n",
            "Epoch: 2/5... Step: 4640... Loss: 1.5860... Val Loss: 1.7201\n",
            "Epoch: 2/5... Step: 4650... Loss: 1.5456... Val Loss: 1.7243\n",
            "Epoch: 2/5... Step: 4660... Loss: 1.6987... Val Loss: 1.7191\n",
            "Epoch: 2/5... Step: 4670... Loss: 1.6059... Val Loss: 1.7195\n",
            "Epoch: 2/5... Step: 4680... Loss: 1.6365... Val Loss: 1.7295\n",
            "Epoch: 2/5... Step: 4690... Loss: 1.6774... Val Loss: 1.7188\n",
            "Epoch: 2/5... Step: 4700... Loss: 1.6200... Val Loss: 1.7145\n",
            "Epoch: 2/5... Step: 4710... Loss: 1.6507... Val Loss: 1.7189\n",
            "Epoch: 2/5... Step: 4720... Loss: 1.6082... Val Loss: 1.7128\n",
            "Epoch: 2/5... Step: 4730... Loss: 1.5930... Val Loss: 1.7233\n",
            "Epoch: 2/5... Step: 4740... Loss: 1.6614... Val Loss: 1.7172\n",
            "Epoch: 2/5... Step: 4750... Loss: 1.6320... Val Loss: 1.7161\n",
            "Epoch: 2/5... Step: 4760... Loss: 1.6254... Val Loss: 1.7153\n",
            "Epoch: 2/5... Step: 4770... Loss: 1.5612... Val Loss: 1.7158\n",
            "Epoch: 2/5... Step: 4780... Loss: 1.6311... Val Loss: 1.7138\n",
            "Epoch: 2/5... Step: 4790... Loss: 1.5701... Val Loss: 1.7177\n",
            "Epoch: 2/5... Step: 4800... Loss: 1.6027... Val Loss: 1.7226\n",
            "Epoch: 2/5... Step: 4810... Loss: 1.6118... Val Loss: 1.7152\n",
            "Epoch: 2/5... Step: 4820... Loss: 1.5898... Val Loss: 1.7175\n",
            "Epoch: 2/5... Step: 4830... Loss: 1.5891... Val Loss: 1.7158\n",
            "Epoch: 2/5... Step: 4840... Loss: 1.6250... Val Loss: 1.7155\n",
            "Epoch: 2/5... Step: 4850... Loss: 1.6584... Val Loss: 1.7123\n",
            "Epoch: 2/5... Step: 4860... Loss: 1.6563... Val Loss: 1.7112\n",
            "Epoch: 2/5... Step: 4870... Loss: 1.6191... Val Loss: 1.7135\n",
            "Epoch: 2/5... Step: 4880... Loss: 1.5811... Val Loss: 1.7115\n",
            "Epoch: 2/5... Step: 4890... Loss: 1.6719... Val Loss: 1.7142\n",
            "Epoch: 2/5... Step: 4900... Loss: 1.5930... Val Loss: 1.7093\n",
            "Epoch: 2/5... Step: 4910... Loss: 1.6111... Val Loss: 1.7099\n",
            "Epoch: 2/5... Step: 4920... Loss: 1.6140... Val Loss: 1.7052\n",
            "Epoch: 2/5... Step: 4930... Loss: 1.6435... Val Loss: 1.7094\n",
            "Epoch: 2/5... Step: 4940... Loss: 1.5893... Val Loss: 1.7072\n",
            "Epoch: 2/5... Step: 4950... Loss: 1.5579... Val Loss: 1.7052\n",
            "Epoch: 2/5... Step: 4960... Loss: 1.6117... Val Loss: 1.7066\n",
            "Epoch: 2/5... Step: 4970... Loss: 1.6632... Val Loss: 1.7070\n",
            "Epoch: 2/5... Step: 4980... Loss: 1.5683... Val Loss: 1.7081\n",
            "Epoch: 2/5... Step: 4990... Loss: 1.6789... Val Loss: 1.7044\n",
            "Epoch: 2/5... Step: 5000... Loss: 1.6572... Val Loss: 1.7050\n",
            "Epoch: 2/5... Step: 5010... Loss: 1.5789... Val Loss: 1.7093\n",
            "Epoch: 2/5... Step: 5020... Loss: 1.5853... Val Loss: 1.7120\n",
            "Epoch: 2/5... Step: 5030... Loss: 1.6272... Val Loss: 1.7070\n",
            "Epoch: 2/5... Step: 5040... Loss: 1.5644... Val Loss: 1.7109\n",
            "Epoch: 2/5... Step: 5050... Loss: 1.5879... Val Loss: 1.7106\n",
            "Epoch: 2/5... Step: 5060... Loss: 1.5308... Val Loss: 1.7099\n",
            "Epoch: 2/5... Step: 5070... Loss: 1.6787... Val Loss: 1.7032\n",
            "Epoch: 2/5... Step: 5080... Loss: 1.6229... Val Loss: 1.7030\n",
            "Epoch: 2/5... Step: 5090... Loss: 1.5966... Val Loss: 1.7044\n",
            "Epoch: 2/5... Step: 5100... Loss: 1.6214... Val Loss: 1.6991\n",
            "Epoch: 2/5... Step: 5110... Loss: 1.6696... Val Loss: 1.7033\n",
            "Epoch: 2/5... Step: 5120... Loss: 1.6076... Val Loss: 1.7025\n",
            "Epoch: 2/5... Step: 5130... Loss: 1.5944... Val Loss: 1.7027\n",
            "Epoch: 2/5... Step: 5140... Loss: 1.6705... Val Loss: 1.7025\n",
            "Epoch: 2/5... Step: 5150... Loss: 1.5535... Val Loss: 1.7013\n",
            "Epoch: 2/5... Step: 5160... Loss: 1.6085... Val Loss: 1.7087\n",
            "Epoch: 2/5... Step: 5170... Loss: 1.5772... Val Loss: 1.7005\n",
            "Epoch: 2/5... Step: 5180... Loss: 1.5438... Val Loss: 1.7036\n",
            "Epoch: 2/5... Step: 5190... Loss: 1.7314... Val Loss: 1.7042\n",
            "Epoch: 2/5... Step: 5200... Loss: 1.6016... Val Loss: 1.7011\n",
            "Epoch: 2/5... Step: 5210... Loss: 1.5015... Val Loss: 1.7008\n",
            "Epoch: 2/5... Step: 5220... Loss: 1.6359... Val Loss: 1.7033\n",
            "Epoch: 2/5... Step: 5230... Loss: 1.7164... Val Loss: 1.7008\n",
            "Epoch: 3/5... Step: 5240... Loss: 1.6426... Val Loss: 1.7114\n",
            "Epoch: 3/5... Step: 5250... Loss: 1.5943... Val Loss: 1.7104\n",
            "Epoch: 3/5... Step: 5260... Loss: 1.6905... Val Loss: 1.6985\n",
            "Epoch: 3/5... Step: 5270... Loss: 1.6436... Val Loss: 1.7010\n",
            "Epoch: 3/5... Step: 5280... Loss: 1.6605... Val Loss: 1.7025\n",
            "Epoch: 3/5... Step: 5290... Loss: 1.6558... Val Loss: 1.6962\n",
            "Epoch: 3/5... Step: 5300... Loss: 1.5963... Val Loss: 1.7004\n",
            "Epoch: 3/5... Step: 5310... Loss: 1.6339... Val Loss: 1.6982\n",
            "Epoch: 3/5... Step: 5320... Loss: 1.6883... Val Loss: 1.6963\n",
            "Epoch: 3/5... Step: 5330... Loss: 1.6309... Val Loss: 1.7042\n",
            "Epoch: 3/5... Step: 5340... Loss: 1.6331... Val Loss: 1.6948\n",
            "Epoch: 3/5... Step: 5350... Loss: 1.6661... Val Loss: 1.6939\n",
            "Epoch: 3/5... Step: 5360... Loss: 1.5389... Val Loss: 1.6963\n",
            "Epoch: 3/5... Step: 5370... Loss: 1.6785... Val Loss: 1.6932\n",
            "Epoch: 3/5... Step: 5380... Loss: 1.5683... Val Loss: 1.6953\n",
            "Epoch: 3/5... Step: 5390... Loss: 1.6841... Val Loss: 1.6943\n",
            "Epoch: 3/5... Step: 5400... Loss: 1.6409... Val Loss: 1.6959\n",
            "Epoch: 3/5... Step: 5410... Loss: 1.6305... Val Loss: 1.6975\n",
            "Epoch: 3/5... Step: 5420... Loss: 1.6271... Val Loss: 1.6934\n",
            "Epoch: 3/5... Step: 5430... Loss: 1.6566... Val Loss: 1.6942\n",
            "Epoch: 3/5... Step: 5440... Loss: 1.6248... Val Loss: 1.7007\n",
            "Epoch: 3/5... Step: 5450... Loss: 1.6372... Val Loss: 1.6924\n",
            "Epoch: 3/5... Step: 5460... Loss: 1.6537... Val Loss: 1.6956\n",
            "Epoch: 3/5... Step: 5470... Loss: 1.6117... Val Loss: 1.7074\n",
            "Epoch: 3/5... Step: 5480... Loss: 1.5741... Val Loss: 1.6950\n",
            "Epoch: 3/5... Step: 5490... Loss: 1.6875... Val Loss: 1.6941\n",
            "Epoch: 3/5... Step: 5500... Loss: 1.5491... Val Loss: 1.6993\n",
            "Epoch: 3/5... Step: 5510... Loss: 1.6424... Val Loss: 1.6932\n",
            "Epoch: 3/5... Step: 5520... Loss: 1.7069... Val Loss: 1.6917\n",
            "Epoch: 3/5... Step: 5530... Loss: 1.5756... Val Loss: 1.6934\n",
            "Epoch: 3/5... Step: 5540... Loss: 1.5846... Val Loss: 1.6911\n",
            "Epoch: 3/5... Step: 5550... Loss: 1.6791... Val Loss: 1.6910\n",
            "Epoch: 3/5... Step: 5560... Loss: 1.6145... Val Loss: 1.6899\n",
            "Epoch: 3/5... Step: 5570... Loss: 1.5593... Val Loss: 1.6896\n",
            "Epoch: 3/5... Step: 5580... Loss: 1.5638... Val Loss: 1.6902\n",
            "Epoch: 3/5... Step: 5590... Loss: 1.6626... Val Loss: 1.6908\n",
            "Epoch: 3/5... Step: 5600... Loss: 1.5177... Val Loss: 1.6863\n",
            "Epoch: 3/5... Step: 5610... Loss: 1.5511... Val Loss: 1.6914\n",
            "Epoch: 3/5... Step: 5620... Loss: 1.6211... Val Loss: 1.6859\n",
            "Epoch: 3/5... Step: 5630... Loss: 1.6728... Val Loss: 1.6871\n",
            "Epoch: 3/5... Step: 5640... Loss: 1.6351... Val Loss: 1.6894\n",
            "Epoch: 3/5... Step: 5650... Loss: 1.6318... Val Loss: 1.6856\n",
            "Epoch: 3/5... Step: 5660... Loss: 1.4754... Val Loss: 1.6858\n",
            "Epoch: 3/5... Step: 5670... Loss: 1.6110... Val Loss: 1.6855\n",
            "Epoch: 3/5... Step: 5680... Loss: 1.5405... Val Loss: 1.6866\n",
            "Epoch: 3/5... Step: 5690... Loss: 1.5341... Val Loss: 1.6848\n",
            "Epoch: 3/5... Step: 5700... Loss: 1.6848... Val Loss: 1.6838\n",
            "Epoch: 3/5... Step: 5710... Loss: 1.5657... Val Loss: 1.6871\n",
            "Epoch: 3/5... Step: 5720... Loss: 1.5097... Val Loss: 1.6883\n",
            "Epoch: 3/5... Step: 5730... Loss: 1.5039... Val Loss: 1.6887\n",
            "Epoch: 3/5... Step: 5740... Loss: 1.6919... Val Loss: 1.6904\n",
            "Epoch: 3/5... Step: 5750... Loss: 1.6040... Val Loss: 1.6847\n",
            "Epoch: 3/5... Step: 5760... Loss: 1.5576... Val Loss: 1.6922\n",
            "Epoch: 3/5... Step: 5770... Loss: 1.5283... Val Loss: 1.6834\n",
            "Epoch: 3/5... Step: 5780... Loss: 1.5860... Val Loss: 1.6882\n",
            "Epoch: 3/5... Step: 5790... Loss: 1.6400... Val Loss: 1.6927\n",
            "Epoch: 3/5... Step: 5800... Loss: 1.6891... Val Loss: 1.6851\n",
            "Epoch: 3/5... Step: 5810... Loss: 1.6488... Val Loss: 1.6895\n",
            "Epoch: 3/5... Step: 5820... Loss: 1.5354... Val Loss: 1.6877\n",
            "Epoch: 3/5... Step: 5830... Loss: 1.5906... Val Loss: 1.6860\n",
            "Epoch: 3/5... Step: 5840... Loss: 1.5843... Val Loss: 1.6885\n",
            "Epoch: 3/5... Step: 5850... Loss: 1.6087... Val Loss: 1.6869\n",
            "Epoch: 3/5... Step: 5860... Loss: 1.5179... Val Loss: 1.6873\n",
            "Epoch: 3/5... Step: 5870... Loss: 1.5299... Val Loss: 1.6820\n",
            "Epoch: 3/5... Step: 5880... Loss: 1.6236... Val Loss: 1.6845\n",
            "Epoch: 3/5... Step: 5890... Loss: 1.6091... Val Loss: 1.6823\n",
            "Epoch: 3/5... Step: 5900... Loss: 1.5743... Val Loss: 1.6828\n",
            "Epoch: 3/5... Step: 5910... Loss: 1.5861... Val Loss: 1.6803\n",
            "Epoch: 3/5... Step: 5920... Loss: 1.6433... Val Loss: 1.6817\n",
            "Epoch: 3/5... Step: 5930... Loss: 1.5412... Val Loss: 1.6854\n",
            "Epoch: 3/5... Step: 5940... Loss: 1.4932... Val Loss: 1.6826\n",
            "Epoch: 3/5... Step: 5950... Loss: 1.6126... Val Loss: 1.6843\n",
            "Epoch: 3/5... Step: 5960... Loss: 1.5451... Val Loss: 1.6811\n",
            "Epoch: 3/5... Step: 5970... Loss: 1.5344... Val Loss: 1.6813\n",
            "Epoch: 3/5... Step: 5980... Loss: 1.5999... Val Loss: 1.6820\n",
            "Epoch: 3/5... Step: 5990... Loss: 1.6615... Val Loss: 1.6770\n",
            "Epoch: 3/5... Step: 6000... Loss: 1.5616... Val Loss: 1.6871\n",
            "Epoch: 3/5... Step: 6010... Loss: 1.6494... Val Loss: 1.6786\n",
            "Epoch: 3/5... Step: 6020... Loss: 1.5786... Val Loss: 1.6775\n",
            "Epoch: 3/5... Step: 6030... Loss: 1.5651... Val Loss: 1.6758\n",
            "Epoch: 3/5... Step: 6040... Loss: 1.6154... Val Loss: 1.6763\n",
            "Epoch: 3/5... Step: 6050... Loss: 1.6732... Val Loss: 1.6801\n",
            "Epoch: 3/5... Step: 6060... Loss: 1.5985... Val Loss: 1.6773\n",
            "Epoch: 3/5... Step: 6070... Loss: 1.5767... Val Loss: 1.6817\n",
            "Epoch: 3/5... Step: 6080... Loss: 1.5927... Val Loss: 1.6783\n",
            "Epoch: 3/5... Step: 6090... Loss: 1.5517... Val Loss: 1.6782\n",
            "Epoch: 3/5... Step: 6100... Loss: 1.5334... Val Loss: 1.6751\n",
            "Epoch: 3/5... Step: 6110... Loss: 1.5427... Val Loss: 1.6732\n",
            "Epoch: 3/5... Step: 6120... Loss: 1.6013... Val Loss: 1.6736\n",
            "Epoch: 3/5... Step: 6130... Loss: 1.6278... Val Loss: 1.6775\n",
            "Epoch: 3/5... Step: 6140... Loss: 1.6100... Val Loss: 1.6766\n",
            "Epoch: 3/5... Step: 6150... Loss: 1.6608... Val Loss: 1.6792\n",
            "Epoch: 3/5... Step: 6160... Loss: 1.6299... Val Loss: 1.6745\n",
            "Epoch: 3/5... Step: 6170... Loss: 1.5679... Val Loss: 1.6740\n",
            "Epoch: 3/5... Step: 6180... Loss: 1.5937... Val Loss: 1.6761\n",
            "Epoch: 3/5... Step: 6190... Loss: 1.6225... Val Loss: 1.6714\n",
            "Epoch: 3/5... Step: 6200... Loss: 1.5029... Val Loss: 1.6722\n",
            "Epoch: 3/5... Step: 6210... Loss: 1.5518... Val Loss: 1.6729\n",
            "Epoch: 3/5... Step: 6220... Loss: 1.5611... Val Loss: 1.6759\n",
            "Epoch: 3/5... Step: 6230... Loss: 1.6759... Val Loss: 1.6768\n",
            "Epoch: 3/5... Step: 6240... Loss: 1.5079... Val Loss: 1.6741\n",
            "Epoch: 3/5... Step: 6250... Loss: 1.5972... Val Loss: 1.6787\n",
            "Epoch: 3/5... Step: 6260... Loss: 1.6172... Val Loss: 1.6723\n",
            "Epoch: 3/5... Step: 6270... Loss: 1.5622... Val Loss: 1.6747\n",
            "Epoch: 3/5... Step: 6280... Loss: 1.4677... Val Loss: 1.6757\n",
            "Epoch: 3/5... Step: 6290... Loss: 1.5936... Val Loss: 1.6721\n",
            "Epoch: 3/5... Step: 6300... Loss: 1.6395... Val Loss: 1.6704\n",
            "Epoch: 3/5... Step: 6310... Loss: 1.5540... Val Loss: 1.6775\n",
            "Epoch: 3/5... Step: 6320... Loss: 1.5432... Val Loss: 1.6712\n",
            "Epoch: 3/5... Step: 6330... Loss: 1.5632... Val Loss: 1.6732\n",
            "Epoch: 3/5... Step: 6340... Loss: 1.6097... Val Loss: 1.6751\n",
            "Epoch: 3/5... Step: 6350... Loss: 1.6131... Val Loss: 1.6729\n",
            "Epoch: 3/5... Step: 6360... Loss: 1.5796... Val Loss: 1.6727\n",
            "Epoch: 3/5... Step: 6370... Loss: 1.6421... Val Loss: 1.6707\n",
            "Epoch: 3/5... Step: 6380... Loss: 1.4868... Val Loss: 1.6742\n",
            "Epoch: 3/5... Step: 6390... Loss: 1.5365... Val Loss: 1.6712\n",
            "Epoch: 3/5... Step: 6400... Loss: 1.6495... Val Loss: 1.6714\n",
            "Epoch: 3/5... Step: 6410... Loss: 1.6158... Val Loss: 1.6744\n",
            "Epoch: 3/5... Step: 6420... Loss: 1.6757... Val Loss: 1.6718\n",
            "Epoch: 3/5... Step: 6430... Loss: 1.5736... Val Loss: 1.6669\n",
            "Epoch: 3/5... Step: 6440... Loss: 1.5569... Val Loss: 1.6724\n",
            "Epoch: 3/5... Step: 6450... Loss: 1.5645... Val Loss: 1.6659\n",
            "Epoch: 3/5... Step: 6460... Loss: 1.6151... Val Loss: 1.6682\n",
            "Epoch: 3/5... Step: 6470... Loss: 1.6086... Val Loss: 1.6677\n",
            "Epoch: 3/5... Step: 6480... Loss: 1.5638... Val Loss: 1.6698\n",
            "Epoch: 3/5... Step: 6490... Loss: 1.5488... Val Loss: 1.6697\n",
            "Epoch: 3/5... Step: 6500... Loss: 1.5460... Val Loss: 1.6655\n",
            "Epoch: 3/5... Step: 6510... Loss: 1.5005... Val Loss: 1.6676\n",
            "Epoch: 3/5... Step: 6520... Loss: 1.5031... Val Loss: 1.6717\n",
            "Epoch: 3/5... Step: 6530... Loss: 1.4762... Val Loss: 1.6704\n",
            "Epoch: 3/5... Step: 6540... Loss: 1.5728... Val Loss: 1.6714\n",
            "Epoch: 3/5... Step: 6550... Loss: 1.5185... Val Loss: 1.6756\n",
            "Epoch: 3/5... Step: 6560... Loss: 1.5156... Val Loss: 1.6661\n",
            "Epoch: 3/5... Step: 6570... Loss: 1.6502... Val Loss: 1.6684\n",
            "Epoch: 3/5... Step: 6580... Loss: 1.5875... Val Loss: 1.6662\n",
            "Epoch: 3/5... Step: 6590... Loss: 1.6579... Val Loss: 1.6660\n",
            "Epoch: 3/5... Step: 6600... Loss: 1.5326... Val Loss: 1.6695\n",
            "Epoch: 3/5... Step: 6610... Loss: 1.6057... Val Loss: 1.6663\n",
            "Epoch: 3/5... Step: 6620... Loss: 1.6000... Val Loss: 1.6643\n",
            "Epoch: 3/5... Step: 6630... Loss: 1.5059... Val Loss: 1.6644\n",
            "Epoch: 3/5... Step: 6640... Loss: 1.5565... Val Loss: 1.6620\n",
            "Epoch: 3/5... Step: 6650... Loss: 1.5602... Val Loss: 1.6667\n",
            "Epoch: 3/5... Step: 6660... Loss: 1.5978... Val Loss: 1.6641\n",
            "Epoch: 3/5... Step: 6670... Loss: 1.5086... Val Loss: 1.6674\n",
            "Epoch: 3/5... Step: 6680... Loss: 1.5768... Val Loss: 1.6645\n",
            "Epoch: 3/5... Step: 6690... Loss: 1.5686... Val Loss: 1.6772\n",
            "Epoch: 3/5... Step: 6700... Loss: 1.5404... Val Loss: 1.6638\n",
            "Epoch: 3/5... Step: 6710... Loss: 1.6155... Val Loss: 1.6676\n",
            "Epoch: 3/5... Step: 6720... Loss: 1.5390... Val Loss: 1.6607\n",
            "Epoch: 3/5... Step: 6730... Loss: 1.5085... Val Loss: 1.6657\n",
            "Epoch: 3/5... Step: 6740... Loss: 1.5710... Val Loss: 1.6645\n",
            "Epoch: 3/5... Step: 6750... Loss: 1.5622... Val Loss: 1.6624\n",
            "Epoch: 3/5... Step: 6760... Loss: 1.5564... Val Loss: 1.6750\n",
            "Epoch: 3/5... Step: 6770... Loss: 1.5600... Val Loss: 1.6615\n",
            "Epoch: 3/5... Step: 6780... Loss: 1.4964... Val Loss: 1.6633\n",
            "Epoch: 3/5... Step: 6790... Loss: 1.5240... Val Loss: 1.6691\n",
            "Epoch: 3/5... Step: 6800... Loss: 1.5717... Val Loss: 1.6621\n",
            "Epoch: 3/5... Step: 6810... Loss: 1.5639... Val Loss: 1.6682\n",
            "Epoch: 3/5... Step: 6820... Loss: 1.6117... Val Loss: 1.6665\n",
            "Epoch: 3/5... Step: 6830... Loss: 1.5683... Val Loss: 1.6635\n",
            "Epoch: 3/5... Step: 6840... Loss: 1.6140... Val Loss: 1.6608\n",
            "Epoch: 3/5... Step: 6850... Loss: 1.6211... Val Loss: 1.6616\n",
            "Epoch: 3/5... Step: 6860... Loss: 1.5622... Val Loss: 1.6603\n",
            "Epoch: 3/5... Step: 6870... Loss: 1.5886... Val Loss: 1.6622\n",
            "Epoch: 3/5... Step: 6880... Loss: 1.5310... Val Loss: 1.6647\n",
            "Epoch: 3/5... Step: 6890... Loss: 1.5930... Val Loss: 1.6611\n",
            "Epoch: 3/5... Step: 6900... Loss: 1.5912... Val Loss: 1.6618\n",
            "Epoch: 3/5... Step: 6910... Loss: 1.5210... Val Loss: 1.6591\n",
            "Epoch: 3/5... Step: 6920... Loss: 1.5439... Val Loss: 1.6628\n",
            "Epoch: 3/5... Step: 6930... Loss: 1.5775... Val Loss: 1.6594\n",
            "Epoch: 3/5... Step: 6940... Loss: 1.6792... Val Loss: 1.6589\n",
            "Epoch: 3/5... Step: 6950... Loss: 1.5547... Val Loss: 1.6590\n",
            "Epoch: 3/5... Step: 6960... Loss: 1.5546... Val Loss: 1.6618\n",
            "Epoch: 3/5... Step: 6970... Loss: 1.5487... Val Loss: 1.6596\n",
            "Epoch: 3/5... Step: 6980... Loss: 1.5892... Val Loss: 1.6649\n",
            "Epoch: 3/5... Step: 6990... Loss: 1.5038... Val Loss: 1.6564\n",
            "Epoch: 3/5... Step: 7000... Loss: 1.5616... Val Loss: 1.6661\n",
            "Epoch: 3/5... Step: 7010... Loss: 1.5673... Val Loss: 1.6617\n",
            "Epoch: 3/5... Step: 7020... Loss: 1.5462... Val Loss: 1.6585\n",
            "Epoch: 3/5... Step: 7030... Loss: 1.4947... Val Loss: 1.6616\n",
            "Epoch: 3/5... Step: 7040... Loss: 1.5601... Val Loss: 1.6590\n",
            "Epoch: 3/5... Step: 7050... Loss: 1.4589... Val Loss: 1.6596\n",
            "Epoch: 3/5... Step: 7060... Loss: 1.4630... Val Loss: 1.6565\n",
            "Epoch: 3/5... Step: 7070... Loss: 1.5430... Val Loss: 1.6576\n",
            "Epoch: 3/5... Step: 7080... Loss: 1.5610... Val Loss: 1.6570\n",
            "Epoch: 3/5... Step: 7090... Loss: 1.5481... Val Loss: 1.6578\n",
            "Epoch: 3/5... Step: 7100... Loss: 1.4595... Val Loss: 1.6575\n",
            "Epoch: 3/5... Step: 7110... Loss: 1.5012... Val Loss: 1.6558\n",
            "Epoch: 3/5... Step: 7120... Loss: 1.6257... Val Loss: 1.6550\n",
            "Epoch: 3/5... Step: 7130... Loss: 1.5554... Val Loss: 1.6560\n",
            "Epoch: 3/5... Step: 7140... Loss: 1.6631... Val Loss: 1.6577\n",
            "Epoch: 3/5... Step: 7150... Loss: 1.6020... Val Loss: 1.6556\n",
            "Epoch: 3/5... Step: 7160... Loss: 1.6063... Val Loss: 1.6590\n",
            "Epoch: 3/5... Step: 7170... Loss: 1.5674... Val Loss: 1.6581\n",
            "Epoch: 3/5... Step: 7180... Loss: 1.6106... Val Loss: 1.6542\n",
            "Epoch: 3/5... Step: 7190... Loss: 1.5941... Val Loss: 1.6537\n",
            "Epoch: 3/5... Step: 7200... Loss: 1.6278... Val Loss: 1.6586\n",
            "Epoch: 3/5... Step: 7210... Loss: 1.5667... Val Loss: 1.6558\n",
            "Epoch: 3/5... Step: 7220... Loss: 1.4684... Val Loss: 1.6663\n",
            "Epoch: 3/5... Step: 7230... Loss: 1.5447... Val Loss: 1.6584\n",
            "Epoch: 3/5... Step: 7240... Loss: 1.4919... Val Loss: 1.6550\n",
            "Epoch: 3/5... Step: 7250... Loss: 1.5768... Val Loss: 1.6643\n",
            "Epoch: 3/5... Step: 7260... Loss: 1.5168... Val Loss: 1.6548\n",
            "Epoch: 3/5... Step: 7270... Loss: 1.5369... Val Loss: 1.6597\n",
            "Epoch: 3/5... Step: 7280... Loss: 1.5367... Val Loss: 1.6528\n",
            "Epoch: 3/5... Step: 7290... Loss: 1.6066... Val Loss: 1.6559\n",
            "Epoch: 3/5... Step: 7300... Loss: 1.5485... Val Loss: 1.6602\n",
            "Epoch: 3/5... Step: 7310... Loss: 1.5148... Val Loss: 1.6540\n",
            "Epoch: 3/5... Step: 7320... Loss: 1.5671... Val Loss: 1.6568\n",
            "Epoch: 3/5... Step: 7330... Loss: 1.4762... Val Loss: 1.6551\n",
            "Epoch: 3/5... Step: 7340... Loss: 1.5524... Val Loss: 1.6520\n",
            "Epoch: 3/5... Step: 7350... Loss: 1.4979... Val Loss: 1.6630\n",
            "Epoch: 3/5... Step: 7360... Loss: 1.5977... Val Loss: 1.6513\n",
            "Epoch: 3/5... Step: 7370... Loss: 1.5805... Val Loss: 1.6509\n",
            "Epoch: 3/5... Step: 7380... Loss: 1.4766... Val Loss: 1.6480\n",
            "Epoch: 3/5... Step: 7390... Loss: 1.5159... Val Loss: 1.6478\n",
            "Epoch: 3/5... Step: 7400... Loss: 1.5663... Val Loss: 1.6498\n",
            "Epoch: 3/5... Step: 7410... Loss: 1.4419... Val Loss: 1.6546\n",
            "Epoch: 3/5... Step: 7420... Loss: 1.5469... Val Loss: 1.6560\n",
            "Epoch: 3/5... Step: 7430... Loss: 1.5080... Val Loss: 1.6511\n",
            "Epoch: 3/5... Step: 7440... Loss: 1.5760... Val Loss: 1.6559\n",
            "Epoch: 3/5... Step: 7450... Loss: 1.5313... Val Loss: 1.6492\n",
            "Epoch: 3/5... Step: 7460... Loss: 1.5997... Val Loss: 1.6518\n",
            "Epoch: 3/5... Step: 7470... Loss: 1.5381... Val Loss: 1.6521\n",
            "Epoch: 3/5... Step: 7480... Loss: 1.5813... Val Loss: 1.6506\n",
            "Epoch: 3/5... Step: 7490... Loss: 1.5425... Val Loss: 1.6505\n",
            "Epoch: 3/5... Step: 7500... Loss: 1.4794... Val Loss: 1.6539\n",
            "Epoch: 3/5... Step: 7510... Loss: 1.5668... Val Loss: 1.6465\n",
            "Epoch: 3/5... Step: 7520... Loss: 1.5511... Val Loss: 1.6467\n",
            "Epoch: 3/5... Step: 7530... Loss: 1.6366... Val Loss: 1.6467\n",
            "Epoch: 3/5... Step: 7540... Loss: 1.5831... Val Loss: 1.6441\n",
            "Epoch: 3/5... Step: 7550... Loss: 1.5356... Val Loss: 1.6453\n",
            "Epoch: 3/5... Step: 7560... Loss: 1.6215... Val Loss: 1.6463\n",
            "Epoch: 3/5... Step: 7570... Loss: 1.4649... Val Loss: 1.6456\n",
            "Epoch: 3/5... Step: 7580... Loss: 1.6023... Val Loss: 1.6449\n",
            "Epoch: 3/5... Step: 7590... Loss: 1.5357... Val Loss: 1.6451\n",
            "Epoch: 3/5... Step: 7600... Loss: 1.5242... Val Loss: 1.6445\n",
            "Epoch: 3/5... Step: 7610... Loss: 1.5249... Val Loss: 1.6446\n",
            "Epoch: 3/5... Step: 7620... Loss: 1.5728... Val Loss: 1.6442\n",
            "Epoch: 3/5... Step: 7630... Loss: 1.5713... Val Loss: 1.6420\n",
            "Epoch: 3/5... Step: 7640... Loss: 1.7216... Val Loss: 1.6429\n",
            "Epoch: 3/5... Step: 7650... Loss: 1.5054... Val Loss: 1.6444\n",
            "Epoch: 3/5... Step: 7660... Loss: 1.6378... Val Loss: 1.6469\n",
            "Epoch: 3/5... Step: 7670... Loss: 1.6131... Val Loss: 1.6454\n",
            "Epoch: 3/5... Step: 7680... Loss: 1.5499... Val Loss: 1.6499\n",
            "Epoch: 3/5... Step: 7690... Loss: 1.5520... Val Loss: 1.6461\n",
            "Epoch: 3/5... Step: 7700... Loss: 1.5519... Val Loss: 1.6449\n",
            "Epoch: 3/5... Step: 7710... Loss: 1.5165... Val Loss: 1.6462\n",
            "Epoch: 3/5... Step: 7720... Loss: 1.5248... Val Loss: 1.6445\n",
            "Epoch: 3/5... Step: 7730... Loss: 1.5966... Val Loss: 1.6459\n",
            "Epoch: 3/5... Step: 7740... Loss: 1.5160... Val Loss: 1.6451\n",
            "Epoch: 3/5... Step: 7750... Loss: 1.5511... Val Loss: 1.6442\n",
            "Epoch: 3/5... Step: 7760... Loss: 1.5267... Val Loss: 1.6465\n",
            "Epoch: 3/5... Step: 7770... Loss: 1.5864... Val Loss: 1.6471\n",
            "Epoch: 3/5... Step: 7780... Loss: 1.5521... Val Loss: 1.6468\n",
            "Epoch: 3/5... Step: 7790... Loss: 1.5524... Val Loss: 1.6422\n",
            "Epoch: 3/5... Step: 7800... Loss: 1.5138... Val Loss: 1.6453\n",
            "Epoch: 3/5... Step: 7810... Loss: 1.6361... Val Loss: 1.6429\n",
            "Epoch: 3/5... Step: 7820... Loss: 1.6334... Val Loss: 1.6406\n",
            "Epoch: 3/5... Step: 7830... Loss: 1.5234... Val Loss: 1.6457\n",
            "Epoch: 3/5... Step: 7840... Loss: 1.5422... Val Loss: 1.6417\n",
            "Epoch: 4/5... Step: 7850... Loss: 1.5900... Val Loss: 1.6554\n",
            "Epoch: 4/5... Step: 7860... Loss: 1.6131... Val Loss: 1.6447\n",
            "Epoch: 4/5... Step: 7870... Loss: 1.5567... Val Loss: 1.6497\n",
            "Epoch: 4/5... Step: 7880... Loss: 1.5335... Val Loss: 1.6444\n",
            "Epoch: 4/5... Step: 7890... Loss: 1.5411... Val Loss: 1.6499\n",
            "Epoch: 4/5... Step: 7900... Loss: 1.5696... Val Loss: 1.6443\n",
            "Epoch: 4/5... Step: 7910... Loss: 1.5467... Val Loss: 1.6438\n",
            "Epoch: 4/5... Step: 7920... Loss: 1.5361... Val Loss: 1.6461\n",
            "Epoch: 4/5... Step: 7930... Loss: 1.5349... Val Loss: 1.6477\n",
            "Epoch: 4/5... Step: 7940... Loss: 1.5377... Val Loss: 1.6437\n",
            "Epoch: 4/5... Step: 7950... Loss: 1.5549... Val Loss: 1.6462\n",
            "Epoch: 4/5... Step: 7960... Loss: 1.5423... Val Loss: 1.6417\n",
            "Epoch: 4/5... Step: 7970... Loss: 1.5575... Val Loss: 1.6425\n",
            "Epoch: 4/5... Step: 7980... Loss: 1.5584... Val Loss: 1.6447\n",
            "Epoch: 4/5... Step: 7990... Loss: 1.5672... Val Loss: 1.6382\n",
            "Epoch: 4/5... Step: 8000... Loss: 1.5750... Val Loss: 1.6423\n",
            "Epoch: 4/5... Step: 8010... Loss: 1.6167... Val Loss: 1.6391\n",
            "Epoch: 4/5... Step: 8020... Loss: 1.6261... Val Loss: 1.6383\n",
            "Epoch: 4/5... Step: 8030... Loss: 1.5938... Val Loss: 1.6428\n",
            "Epoch: 4/5... Step: 8040... Loss: 1.5505... Val Loss: 1.6375\n",
            "Epoch: 4/5... Step: 8050... Loss: 1.6454... Val Loss: 1.6374\n",
            "Epoch: 4/5... Step: 8060... Loss: 1.5020... Val Loss: 1.6417\n",
            "Epoch: 4/5... Step: 8070... Loss: 1.3999... Val Loss: 1.6367\n",
            "Epoch: 4/5... Step: 8080... Loss: 1.5739... Val Loss: 1.6414\n",
            "Epoch: 4/5... Step: 8090... Loss: 1.4472... Val Loss: 1.6387\n",
            "Epoch: 4/5... Step: 8100... Loss: 1.5379... Val Loss: 1.6436\n",
            "Epoch: 4/5... Step: 8110... Loss: 1.5111... Val Loss: 1.6397\n",
            "Epoch: 4/5... Step: 8120... Loss: 1.5764... Val Loss: 1.6384\n",
            "Epoch: 4/5... Step: 8130... Loss: 1.6273... Val Loss: 1.6397\n",
            "Epoch: 4/5... Step: 8140... Loss: 1.5724... Val Loss: 1.6411\n",
            "Epoch: 4/5... Step: 8150... Loss: 1.5710... Val Loss: 1.6409\n",
            "Epoch: 4/5... Step: 8160... Loss: 1.5855... Val Loss: 1.6429\n",
            "Epoch: 4/5... Step: 8170... Loss: 1.4986... Val Loss: 1.6366\n",
            "Epoch: 4/5... Step: 8180... Loss: 1.5849... Val Loss: 1.6415\n",
            "Epoch: 4/5... Step: 8190... Loss: 1.6009... Val Loss: 1.6411\n",
            "Epoch: 4/5... Step: 8200... Loss: 1.5922... Val Loss: 1.6394\n",
            "Epoch: 4/5... Step: 8210... Loss: 1.5976... Val Loss: 1.6396\n",
            "Epoch: 4/5... Step: 8220... Loss: 1.5462... Val Loss: 1.6365\n",
            "Epoch: 4/5... Step: 8230... Loss: 1.4935... Val Loss: 1.6410\n",
            "Epoch: 4/5... Step: 8240... Loss: 1.5668... Val Loss: 1.6339\n",
            "Epoch: 4/5... Step: 8250... Loss: 1.6084... Val Loss: 1.6353\n",
            "Epoch: 4/5... Step: 8260... Loss: 1.4824... Val Loss: 1.6352\n",
            "Epoch: 4/5... Step: 8270... Loss: 1.4555... Val Loss: 1.6327\n",
            "Epoch: 4/5... Step: 8280... Loss: 1.4576... Val Loss: 1.6340\n",
            "Epoch: 4/5... Step: 8290... Loss: 1.4799... Val Loss: 1.6350\n",
            "Epoch: 4/5... Step: 8300... Loss: 1.5592... Val Loss: 1.6334\n",
            "Epoch: 4/5... Step: 8310... Loss: 1.4292... Val Loss: 1.6337\n",
            "Epoch: 4/5... Step: 8320... Loss: 1.4513... Val Loss: 1.6351\n",
            "Epoch: 4/5... Step: 8330... Loss: 1.4756... Val Loss: 1.6383\n",
            "Epoch: 4/5... Step: 8340... Loss: 1.5088... Val Loss: 1.6367\n",
            "Epoch: 4/5... Step: 8350... Loss: 1.5574... Val Loss: 1.6380\n",
            "Epoch: 4/5... Step: 8360... Loss: 1.5636... Val Loss: 1.6382\n",
            "Epoch: 4/5... Step: 8370... Loss: 1.4692... Val Loss: 1.6355\n",
            "Epoch: 4/5... Step: 8380... Loss: 1.4406... Val Loss: 1.6425\n",
            "Epoch: 4/5... Step: 8390... Loss: 1.5651... Val Loss: 1.6338\n",
            "Epoch: 4/5... Step: 8400... Loss: 1.5802... Val Loss: 1.6382\n",
            "Epoch: 4/5... Step: 8410... Loss: 1.5674... Val Loss: 1.6334\n",
            "Epoch: 4/5... Step: 8420... Loss: 1.5407... Val Loss: 1.6432\n",
            "Epoch: 4/5... Step: 8430... Loss: 1.4826... Val Loss: 1.6372\n",
            "Epoch: 4/5... Step: 8440... Loss: 1.5355... Val Loss: 1.6367\n",
            "Epoch: 4/5... Step: 8450... Loss: 1.5949... Val Loss: 1.6335\n",
            "Epoch: 4/5... Step: 8460... Loss: 1.5433... Val Loss: 1.6380\n",
            "Epoch: 4/5... Step: 8470... Loss: 1.4945... Val Loss: 1.6359\n",
            "Epoch: 4/5... Step: 8480... Loss: 1.5063... Val Loss: 1.6356\n",
            "Epoch: 4/5... Step: 8490... Loss: 1.4773... Val Loss: 1.6329\n",
            "Epoch: 4/5... Step: 8500... Loss: 1.5750... Val Loss: 1.6322\n",
            "Epoch: 4/5... Step: 8510... Loss: 1.5187... Val Loss: 1.6328\n",
            "Epoch: 4/5... Step: 8520... Loss: 1.6270... Val Loss: 1.6374\n",
            "Epoch: 4/5... Step: 8530... Loss: 1.5419... Val Loss: 1.6313\n",
            "Epoch: 4/5... Step: 8540... Loss: 1.5434... Val Loss: 1.6318\n",
            "Epoch: 4/5... Step: 8550... Loss: 1.6077... Val Loss: 1.6370\n",
            "Epoch: 4/5... Step: 8560... Loss: 1.4726... Val Loss: 1.6349\n",
            "Epoch: 4/5... Step: 8570... Loss: 1.5576... Val Loss: 1.6342\n",
            "Epoch: 4/5... Step: 8580... Loss: 1.4744... Val Loss: 1.6333\n",
            "Epoch: 4/5... Step: 8590... Loss: 1.5048... Val Loss: 1.6341\n",
            "Epoch: 4/5... Step: 8600... Loss: 1.5578... Val Loss: 1.6374\n",
            "Epoch: 4/5... Step: 8610... Loss: 1.5452... Val Loss: 1.6329\n",
            "Epoch: 4/5... Step: 8620... Loss: 1.5335... Val Loss: 1.6363\n",
            "Epoch: 4/5... Step: 8630... Loss: 1.5899... Val Loss: 1.6306\n",
            "Epoch: 4/5... Step: 8640... Loss: 1.4896... Val Loss: 1.6311\n",
            "Epoch: 4/5... Step: 8650... Loss: 1.5163... Val Loss: 1.6307\n",
            "Epoch: 4/5... Step: 8660... Loss: 1.5179... Val Loss: 1.6323\n",
            "Epoch: 4/5... Step: 8670... Loss: 1.5626... Val Loss: 1.6317\n",
            "Epoch: 4/5... Step: 8680... Loss: 1.4943... Val Loss: 1.6362\n",
            "Epoch: 4/5... Step: 8690... Loss: 1.5894... Val Loss: 1.6317\n",
            "Epoch: 4/5... Step: 8700... Loss: 1.4742... Val Loss: 1.6321\n",
            "Epoch: 4/5... Step: 8710... Loss: 1.5167... Val Loss: 1.6332\n",
            "Epoch: 4/5... Step: 8720... Loss: 1.4592... Val Loss: 1.6308\n",
            "Epoch: 4/5... Step: 8730... Loss: 1.5435... Val Loss: 1.6305\n",
            "Epoch: 4/5... Step: 8740... Loss: 1.4511... Val Loss: 1.6318\n",
            "Epoch: 4/5... Step: 8750... Loss: 1.5716... Val Loss: 1.6297\n",
            "Epoch: 4/5... Step: 8760... Loss: 1.5666... Val Loss: 1.6293\n",
            "Epoch: 4/5... Step: 8770... Loss: 1.4797... Val Loss: 1.6303\n",
            "Epoch: 4/5... Step: 8780... Loss: 1.5183... Val Loss: 1.6302\n",
            "Epoch: 4/5... Step: 8790... Loss: 1.5792... Val Loss: 1.6296\n",
            "Epoch: 4/5... Step: 8800... Loss: 1.4710... Val Loss: 1.6257\n",
            "Epoch: 4/5... Step: 8810... Loss: 1.5623... Val Loss: 1.6259\n",
            "Epoch: 4/5... Step: 8820... Loss: 1.5467... Val Loss: 1.6264\n",
            "Epoch: 4/5... Step: 8830... Loss: 1.3948... Val Loss: 1.6254\n",
            "Epoch: 4/5... Step: 8840... Loss: 1.5587... Val Loss: 1.6264\n",
            "Epoch: 4/5... Step: 8850... Loss: 1.5209... Val Loss: 1.6245\n",
            "Epoch: 4/5... Step: 8860... Loss: 1.5184... Val Loss: 1.6282\n",
            "Epoch: 4/5... Step: 8870... Loss: 1.5064... Val Loss: 1.6264\n",
            "Epoch: 4/5... Step: 8880... Loss: 1.5441... Val Loss: 1.6262\n",
            "Epoch: 4/5... Step: 8890... Loss: 1.4840... Val Loss: 1.6310\n",
            "Epoch: 4/5... Step: 8900... Loss: 1.4832... Val Loss: 1.6260\n",
            "Epoch: 4/5... Step: 8910... Loss: 1.5177... Val Loss: 1.6278\n",
            "Epoch: 4/5... Step: 8920... Loss: 1.4765... Val Loss: 1.6279\n",
            "Epoch: 4/5... Step: 8930... Loss: 1.4654... Val Loss: 1.6325\n",
            "Epoch: 4/5... Step: 8940... Loss: 1.5690... Val Loss: 1.6276\n",
            "Epoch: 4/5... Step: 8950... Loss: 1.6054... Val Loss: 1.6261\n",
            "Epoch: 4/5... Step: 8960... Loss: 1.4755... Val Loss: 1.6258\n",
            "Epoch: 4/5... Step: 8970... Loss: 1.4534... Val Loss: 1.6240\n",
            "Epoch: 4/5... Step: 8980... Loss: 1.5238... Val Loss: 1.6316\n",
            "Epoch: 4/5... Step: 8990... Loss: 1.5988... Val Loss: 1.6248\n",
            "Epoch: 4/5... Step: 9000... Loss: 1.5426... Val Loss: 1.6253\n",
            "Epoch: 4/5... Step: 9010... Loss: 1.6519... Val Loss: 1.6277\n",
            "Epoch: 4/5... Step: 9020... Loss: 1.5875... Val Loss: 1.6245\n",
            "Epoch: 4/5... Step: 9030... Loss: 1.3779... Val Loss: 1.6286\n",
            "Epoch: 4/5... Step: 9040... Loss: 1.4769... Val Loss: 1.6293\n",
            "Epoch: 4/5... Step: 9050... Loss: 1.5205... Val Loss: 1.6223\n",
            "Epoch: 4/5... Step: 9060... Loss: 1.5083... Val Loss: 1.6240\n",
            "Epoch: 4/5... Step: 9070... Loss: 1.5299... Val Loss: 1.6256\n",
            "Epoch: 4/5... Step: 9080... Loss: 1.5564... Val Loss: 1.6279\n",
            "Epoch: 4/5... Step: 9090... Loss: 1.5566... Val Loss: 1.6234\n",
            "Epoch: 4/5... Step: 9100... Loss: 1.6090... Val Loss: 1.6266\n",
            "Epoch: 4/5... Step: 9110... Loss: 1.4425... Val Loss: 1.6353\n",
            "Epoch: 4/5... Step: 9120... Loss: 1.5097... Val Loss: 1.6234\n",
            "Epoch: 4/5... Step: 9130... Loss: 1.4748... Val Loss: 1.6237\n",
            "Epoch: 4/5... Step: 9140... Loss: 1.5476... Val Loss: 1.6281\n",
            "Epoch: 4/5... Step: 9150... Loss: 1.4818... Val Loss: 1.6247\n",
            "Epoch: 4/5... Step: 9160... Loss: 1.5179... Val Loss: 1.6244\n",
            "Epoch: 4/5... Step: 9170... Loss: 1.4595... Val Loss: 1.6282\n",
            "Epoch: 4/5... Step: 9180... Loss: 1.5357... Val Loss: 1.6264\n",
            "Epoch: 4/5... Step: 9190... Loss: 1.5203... Val Loss: 1.6248\n",
            "Epoch: 4/5... Step: 9200... Loss: 1.4787... Val Loss: 1.6243\n",
            "Epoch: 4/5... Step: 9210... Loss: 1.5321... Val Loss: 1.6239\n",
            "Epoch: 4/5... Step: 9220... Loss: 1.4952... Val Loss: 1.6258\n",
            "Epoch: 4/5... Step: 9230... Loss: 1.5726... Val Loss: 1.6277\n",
            "Epoch: 4/5... Step: 9240... Loss: 1.4610... Val Loss: 1.6277\n",
            "Epoch: 4/5... Step: 9250... Loss: 1.5650... Val Loss: 1.6230\n",
            "Epoch: 4/5... Step: 9260... Loss: 1.5255... Val Loss: 1.6206\n",
            "Epoch: 4/5... Step: 9270... Loss: 1.5627... Val Loss: 1.6237\n",
            "Epoch: 4/5... Step: 9280... Loss: 1.5287... Val Loss: 1.6238\n",
            "Epoch: 4/5... Step: 9290... Loss: 1.5257... Val Loss: 1.6235\n",
            "Epoch: 4/5... Step: 9300... Loss: 1.4847... Val Loss: 1.6265\n",
            "Epoch: 4/5... Step: 9310... Loss: 1.5304... Val Loss: 1.6265\n",
            "Epoch: 4/5... Step: 9320... Loss: 1.4735... Val Loss: 1.6262\n",
            "Epoch: 4/5... Step: 9330... Loss: 1.4312... Val Loss: 1.6253\n",
            "Epoch: 4/5... Step: 9340... Loss: 1.4859... Val Loss: 1.6216\n",
            "Epoch: 4/5... Step: 9350... Loss: 1.4308... Val Loss: 1.6294\n",
            "Epoch: 4/5... Step: 9360... Loss: 1.4859... Val Loss: 1.6249\n",
            "Epoch: 4/5... Step: 9370... Loss: 1.4538... Val Loss: 1.6246\n",
            "Epoch: 4/5... Step: 9380... Loss: 1.5057... Val Loss: 1.6346\n",
            "Epoch: 4/5... Step: 9390... Loss: 1.5226... Val Loss: 1.6223\n",
            "Epoch: 4/5... Step: 9400... Loss: 1.5513... Val Loss: 1.6269\n",
            "Epoch: 4/5... Step: 9410... Loss: 1.5469... Val Loss: 1.6322\n",
            "Epoch: 4/5... Step: 9420... Loss: 1.4137... Val Loss: 1.6275\n",
            "Epoch: 4/5... Step: 9430... Loss: 1.4846... Val Loss: 1.6321\n",
            "Epoch: 4/5... Step: 9440... Loss: 1.5319... Val Loss: 1.6229\n",
            "Epoch: 4/5... Step: 9450... Loss: 1.6148... Val Loss: 1.6328\n",
            "Epoch: 4/5... Step: 9460... Loss: 1.5061... Val Loss: 1.6213\n",
            "Epoch: 4/5... Step: 9470... Loss: 1.5226... Val Loss: 1.6266\n",
            "Epoch: 4/5... Step: 9480... Loss: 1.4769... Val Loss: 1.6234\n",
            "Epoch: 4/5... Step: 9490... Loss: 1.5045... Val Loss: 1.6278\n",
            "Epoch: 4/5... Step: 9500... Loss: 1.4930... Val Loss: 1.6215\n",
            "Epoch: 4/5... Step: 9510... Loss: 1.4268... Val Loss: 1.6194\n",
            "Epoch: 4/5... Step: 9520... Loss: 1.6185... Val Loss: 1.6209\n",
            "Epoch: 4/5... Step: 9530... Loss: 1.5327... Val Loss: 1.6186\n",
            "Epoch: 4/5... Step: 9540... Loss: 1.5133... Val Loss: 1.6256\n",
            "Epoch: 4/5... Step: 9550... Loss: 1.5149... Val Loss: 1.6180\n",
            "Epoch: 4/5... Step: 9560... Loss: 1.5082... Val Loss: 1.6194\n",
            "Epoch: 4/5... Step: 9570... Loss: 1.5211... Val Loss: 1.6244\n",
            "Epoch: 4/5... Step: 9580... Loss: 1.5355... Val Loss: 1.6200\n",
            "Epoch: 4/5... Step: 9590... Loss: 1.5357... Val Loss: 1.6240\n",
            "Epoch: 4/5... Step: 9600... Loss: 1.5390... Val Loss: 1.6256\n",
            "Epoch: 4/5... Step: 9610... Loss: 1.5629... Val Loss: 1.6176\n",
            "Epoch: 4/5... Step: 9620... Loss: 1.5253... Val Loss: 1.6247\n",
            "Epoch: 4/5... Step: 9630... Loss: 1.5160... Val Loss: 1.6206\n",
            "Epoch: 4/5... Step: 9640... Loss: 1.5434... Val Loss: 1.6253\n",
            "Epoch: 4/5... Step: 9650... Loss: 1.5193... Val Loss: 1.6221\n",
            "Epoch: 4/5... Step: 9660... Loss: 1.4946... Val Loss: 1.6267\n",
            "Epoch: 4/5... Step: 9670... Loss: 1.5584... Val Loss: 1.6204\n",
            "Epoch: 4/5... Step: 9680... Loss: 1.4612... Val Loss: 1.6180\n",
            "Epoch: 4/5... Step: 9690... Loss: 1.5624... Val Loss: 1.6165\n",
            "Epoch: 4/5... Step: 9700... Loss: 1.5346... Val Loss: 1.6229\n",
            "Epoch: 4/5... Step: 9710... Loss: 1.5355... Val Loss: 1.6188\n",
            "Epoch: 4/5... Step: 9720... Loss: 1.5115... Val Loss: 1.6198\n",
            "Epoch: 4/5... Step: 9730... Loss: 1.4422... Val Loss: 1.6200\n",
            "Epoch: 4/5... Step: 9740... Loss: 1.5544... Val Loss: 1.6206\n",
            "Epoch: 4/5... Step: 9750... Loss: 1.5194... Val Loss: 1.6216\n",
            "Epoch: 4/5... Step: 9760... Loss: 1.4853... Val Loss: 1.6176\n",
            "Epoch: 4/5... Step: 9770... Loss: 1.5272... Val Loss: 1.6167\n",
            "Epoch: 4/5... Step: 9780... Loss: 1.4452... Val Loss: 1.6187\n",
            "Epoch: 4/5... Step: 9790... Loss: 1.5674... Val Loss: 1.6200\n",
            "Epoch: 4/5... Step: 9800... Loss: 1.5468... Val Loss: 1.6198\n",
            "Epoch: 4/5... Step: 9810... Loss: 1.5313... Val Loss: 1.6168\n",
            "Epoch: 4/5... Step: 9820... Loss: 1.4296... Val Loss: 1.6194\n",
            "Epoch: 4/5... Step: 9830... Loss: 1.5062... Val Loss: 1.6202\n",
            "Epoch: 4/5... Step: 9840... Loss: 1.3966... Val Loss: 1.6285\n",
            "Epoch: 4/5... Step: 9850... Loss: 1.4699... Val Loss: 1.6213\n",
            "Epoch: 4/5... Step: 9860... Loss: 1.4674... Val Loss: 1.6213\n",
            "Epoch: 4/5... Step: 9870... Loss: 1.5203... Val Loss: 1.6226\n",
            "Epoch: 4/5... Step: 9880... Loss: 1.4453... Val Loss: 1.6195\n",
            "Epoch: 4/5... Step: 9890... Loss: 1.4792... Val Loss: 1.6177\n",
            "Epoch: 4/5... Step: 9900... Loss: 1.5460... Val Loss: 1.6179\n",
            "Epoch: 4/5... Step: 9910... Loss: 1.4991... Val Loss: 1.6197\n",
            "Epoch: 4/5... Step: 9920... Loss: 1.5616... Val Loss: 1.6247\n",
            "Epoch: 4/5... Step: 9930... Loss: 1.5139... Val Loss: 1.6176\n",
            "Epoch: 4/5... Step: 9940... Loss: 1.4242... Val Loss: 1.6197\n",
            "Epoch: 4/5... Step: 9950... Loss: 1.4480... Val Loss: 1.6168\n",
            "Epoch: 4/5... Step: 9960... Loss: 1.4774... Val Loss: 1.6183\n",
            "Epoch: 4/5... Step: 9970... Loss: 1.5064... Val Loss: 1.6169\n",
            "Epoch: 4/5... Step: 9980... Loss: 1.5399... Val Loss: 1.6151\n",
            "Epoch: 4/5... Step: 9990... Loss: 1.6015... Val Loss: 1.6146\n",
            "Epoch: 4/5... Step: 10000... Loss: 1.4900... Val Loss: 1.6174\n",
            "Epoch: 4/5... Step: 10010... Loss: 1.4617... Val Loss: 1.6142\n",
            "Epoch: 4/5... Step: 10020... Loss: 1.4629... Val Loss: 1.6137\n",
            "Epoch: 4/5... Step: 10030... Loss: 1.4462... Val Loss: 1.6216\n",
            "Epoch: 4/5... Step: 10040... Loss: 1.3979... Val Loss: 1.6150\n",
            "Epoch: 4/5... Step: 10050... Loss: 1.3652... Val Loss: 1.6161\n",
            "Epoch: 4/5... Step: 10060... Loss: 1.5109... Val Loss: 1.6167\n",
            "Epoch: 4/5... Step: 10070... Loss: 1.4593... Val Loss: 1.6169\n",
            "Epoch: 4/5... Step: 10080... Loss: 1.5686... Val Loss: 1.6208\n",
            "Epoch: 4/5... Step: 10090... Loss: 1.5384... Val Loss: 1.6186\n",
            "Epoch: 4/5... Step: 10100... Loss: 1.5006... Val Loss: 1.6185\n",
            "Epoch: 4/5... Step: 10110... Loss: 1.4990... Val Loss: 1.6168\n",
            "Epoch: 4/5... Step: 10120... Loss: 1.5212... Val Loss: 1.6152\n",
            "Epoch: 4/5... Step: 10130... Loss: 1.5878... Val Loss: 1.6099\n",
            "Epoch: 4/5... Step: 10140... Loss: 1.5410... Val Loss: 1.6095\n",
            "Epoch: 4/5... Step: 10150... Loss: 1.4997... Val Loss: 1.6100\n",
            "Epoch: 4/5... Step: 10160... Loss: 1.5467... Val Loss: 1.6108\n",
            "Epoch: 4/5... Step: 10170... Loss: 1.5189... Val Loss: 1.6107\n",
            "Epoch: 4/5... Step: 10180... Loss: 1.4808... Val Loss: 1.6104\n",
            "Epoch: 4/5... Step: 10190... Loss: 1.4915... Val Loss: 1.6104\n",
            "Epoch: 4/5... Step: 10200... Loss: 1.5983... Val Loss: 1.6097\n",
            "Epoch: 4/5... Step: 10210... Loss: 1.5078... Val Loss: 1.6101\n",
            "Epoch: 4/5... Step: 10220... Loss: 1.5094... Val Loss: 1.6129\n",
            "Epoch: 4/5... Step: 10230... Loss: 1.5741... Val Loss: 1.6120\n",
            "Epoch: 4/5... Step: 10240... Loss: 1.3727... Val Loss: 1.6087\n",
            "Epoch: 4/5... Step: 10250... Loss: 1.4477... Val Loss: 1.6086\n",
            "Epoch: 4/5... Step: 10260... Loss: 1.4230... Val Loss: 1.6084\n",
            "Epoch: 4/5... Step: 10270... Loss: 1.5021... Val Loss: 1.6134\n",
            "Epoch: 4/5... Step: 10280... Loss: 1.4737... Val Loss: 1.6072\n",
            "Epoch: 4/5... Step: 10290... Loss: 1.4933... Val Loss: 1.6100\n",
            "Epoch: 4/5... Step: 10300... Loss: 1.5357... Val Loss: 1.6106\n",
            "Epoch: 4/5... Step: 10310... Loss: 1.5540... Val Loss: 1.6081\n",
            "Epoch: 4/5... Step: 10320... Loss: 1.4912... Val Loss: 1.6080\n",
            "Epoch: 4/5... Step: 10330... Loss: 1.5788... Val Loss: 1.6080\n",
            "Epoch: 4/5... Step: 10340... Loss: 1.5703... Val Loss: 1.6067\n",
            "Epoch: 4/5... Step: 10350... Loss: 1.5432... Val Loss: 1.6077\n",
            "Epoch: 4/5... Step: 10360... Loss: 1.4702... Val Loss: 1.6073\n",
            "Epoch: 4/5... Step: 10370... Loss: 1.5230... Val Loss: 1.6082\n",
            "Epoch: 4/5... Step: 10380... Loss: 1.5061... Val Loss: 1.6084\n",
            "Epoch: 4/5... Step: 10390... Loss: 1.4872... Val Loss: 1.6113\n",
            "Epoch: 4/5... Step: 10400... Loss: 1.4918... Val Loss: 1.6087\n",
            "Epoch: 4/5... Step: 10410... Loss: 1.5188... Val Loss: 1.6070\n",
            "Epoch: 4/5... Step: 10420... Loss: 1.4471... Val Loss: 1.6122\n",
            "Epoch: 4/5... Step: 10430... Loss: 1.5973... Val Loss: 1.6069\n",
            "Epoch: 4/5... Step: 10440... Loss: 1.4459... Val Loss: 1.6067\n",
            "Epoch: 4/5... Step: 10450... Loss: 1.5780... Val Loss: 1.6093\n",
            "Epoch: 4/5... Step: 10460... Loss: 1.5298... Val Loss: 1.6049\n",
            "Epoch: 5/5... Step: 10470... Loss: 1.5675... Val Loss: 1.6267\n",
            "Epoch: 5/5... Step: 10480... Loss: 1.5980... Val Loss: 1.6161\n",
            "Epoch: 5/5... Step: 10490... Loss: 1.5182... Val Loss: 1.6105\n",
            "Epoch: 5/5... Step: 10500... Loss: 1.5740... Val Loss: 1.6088\n",
            "Epoch: 5/5... Step: 10510... Loss: 1.5090... Val Loss: 1.6120\n",
            "Epoch: 5/5... Step: 10520... Loss: 1.4872... Val Loss: 1.6096\n",
            "Epoch: 5/5... Step: 10530... Loss: 1.5307... Val Loss: 1.6103\n",
            "Epoch: 5/5... Step: 10540... Loss: 1.4450... Val Loss: 1.6153\n",
            "Epoch: 5/5... Step: 10550... Loss: 1.6530... Val Loss: 1.6128\n",
            "Epoch: 5/5... Step: 10560... Loss: 1.4633... Val Loss: 1.6088\n",
            "Epoch: 5/5... Step: 10570... Loss: 1.4638... Val Loss: 1.6111\n",
            "Epoch: 5/5... Step: 10580... Loss: 1.5749... Val Loss: 1.6065\n",
            "Epoch: 5/5... Step: 10590... Loss: 1.4841... Val Loss: 1.6050\n",
            "Epoch: 5/5... Step: 10600... Loss: 1.4852... Val Loss: 1.6048\n",
            "Epoch: 5/5... Step: 10610... Loss: 1.5034... Val Loss: 1.6051\n",
            "Epoch: 5/5... Step: 10620... Loss: 1.4305... Val Loss: 1.6056\n",
            "Epoch: 5/5... Step: 10630... Loss: 1.5603... Val Loss: 1.6028\n",
            "Epoch: 5/5... Step: 10640... Loss: 1.5764... Val Loss: 1.6037\n",
            "Epoch: 5/5... Step: 10650... Loss: 1.5852... Val Loss: 1.6056\n",
            "Epoch: 5/5... Step: 10660... Loss: 1.4999... Val Loss: 1.6057\n",
            "Epoch: 5/5... Step: 10670... Loss: 1.5344... Val Loss: 1.6076\n",
            "Epoch: 5/5... Step: 10680... Loss: 1.4579... Val Loss: 1.6069\n",
            "Epoch: 5/5... Step: 10690... Loss: 1.5032... Val Loss: 1.6102\n",
            "Epoch: 5/5... Step: 10700... Loss: 1.5490... Val Loss: 1.6054\n",
            "Epoch: 5/5... Step: 10710... Loss: 1.4983... Val Loss: 1.6117\n",
            "Epoch: 5/5... Step: 10720... Loss: 1.4698... Val Loss: 1.6120\n",
            "Epoch: 5/5... Step: 10730... Loss: 1.4938... Val Loss: 1.6071\n",
            "Epoch: 5/5... Step: 10740... Loss: 1.4537... Val Loss: 1.6073\n",
            "Epoch: 5/5... Step: 10750... Loss: 1.5473... Val Loss: 1.6079\n",
            "Epoch: 5/5... Step: 10760... Loss: 1.4825... Val Loss: 1.6064\n",
            "Epoch: 5/5... Step: 10770... Loss: 1.4543... Val Loss: 1.6086\n",
            "Epoch: 5/5... Step: 10780... Loss: 1.5751... Val Loss: 1.6086\n",
            "Epoch: 5/5... Step: 10790... Loss: 1.4904... Val Loss: 1.6042\n",
            "Epoch: 5/5... Step: 10800... Loss: 1.5138... Val Loss: 1.6063\n",
            "Epoch: 5/5... Step: 10810... Loss: 1.4852... Val Loss: 1.6046\n",
            "Epoch: 5/5... Step: 10820... Loss: 1.4982... Val Loss: 1.6101\n",
            "Epoch: 5/5... Step: 10830... Loss: 1.5477... Val Loss: 1.6022\n",
            "Epoch: 5/5... Step: 10840... Loss: 1.5475... Val Loss: 1.6057\n",
            "Epoch: 5/5... Step: 10850... Loss: 1.4123... Val Loss: 1.6066\n",
            "Epoch: 5/5... Step: 10860... Loss: 1.5021... Val Loss: 1.6024\n",
            "Epoch: 5/5... Step: 10870... Loss: 1.4690... Val Loss: 1.6061\n",
            "Epoch: 5/5... Step: 10880... Loss: 1.5317... Val Loss: 1.6047\n",
            "Epoch: 5/5... Step: 10890... Loss: 1.4543... Val Loss: 1.6023\n",
            "Epoch: 5/5... Step: 10900... Loss: 1.4563... Val Loss: 1.6029\n",
            "Epoch: 5/5... Step: 10910... Loss: 1.4548... Val Loss: 1.6038\n",
            "Epoch: 5/5... Step: 10920... Loss: 1.5107... Val Loss: 1.6024\n",
            "Epoch: 5/5... Step: 10930... Loss: 1.4463... Val Loss: 1.6035\n",
            "Epoch: 5/5... Step: 10940... Loss: 1.4518... Val Loss: 1.6054\n",
            "Epoch: 5/5... Step: 10950... Loss: 1.5187... Val Loss: 1.6078\n",
            "Epoch: 5/5... Step: 10960... Loss: 1.5026... Val Loss: 1.6065\n",
            "Epoch: 5/5... Step: 10970... Loss: 1.4941... Val Loss: 1.6078\n",
            "Epoch: 5/5... Step: 10980... Loss: 1.4980... Val Loss: 1.6042\n",
            "Epoch: 5/5... Step: 10990... Loss: 1.4372... Val Loss: 1.6045\n",
            "Epoch: 5/5... Step: 11000... Loss: 1.5099... Val Loss: 1.6117\n",
            "Epoch: 5/5... Step: 11010... Loss: 1.5774... Val Loss: 1.6032\n",
            "Epoch: 5/5... Step: 11020... Loss: 1.5350... Val Loss: 1.6040\n",
            "Epoch: 5/5... Step: 11030... Loss: 1.5349... Val Loss: 1.6024\n",
            "Epoch: 5/5... Step: 11040... Loss: 1.5661... Val Loss: 1.6106\n",
            "Epoch: 5/5... Step: 11050... Loss: 1.3960... Val Loss: 1.6069\n",
            "Epoch: 5/5... Step: 11060... Loss: 1.5075... Val Loss: 1.6068\n",
            "Epoch: 5/5... Step: 11070... Loss: 1.4709... Val Loss: 1.6049\n",
            "Epoch: 5/5... Step: 11080... Loss: 1.4879... Val Loss: 1.6098\n",
            "Epoch: 5/5... Step: 11090... Loss: 1.4247... Val Loss: 1.6051\n",
            "Epoch: 5/5... Step: 11100... Loss: 1.4086... Val Loss: 1.6049\n",
            "Epoch: 5/5... Step: 11110... Loss: 1.4392... Val Loss: 1.6017\n",
            "Epoch: 5/5... Step: 11120... Loss: 1.5506... Val Loss: 1.6017\n",
            "Epoch: 5/5... Step: 11130... Loss: 1.5068... Val Loss: 1.6034\n",
            "Epoch: 5/5... Step: 11140... Loss: 1.4910... Val Loss: 1.6025\n",
            "Epoch: 5/5... Step: 11150... Loss: 1.4450... Val Loss: 1.6033\n",
            "Epoch: 5/5... Step: 11160... Loss: 1.6178... Val Loss: 1.6022\n",
            "Epoch: 5/5... Step: 11170... Loss: 1.5056... Val Loss: 1.6027\n",
            "Epoch: 5/5... Step: 11180... Loss: 1.4520... Val Loss: 1.6069\n",
            "Epoch: 5/5... Step: 11190... Loss: 1.4946... Val Loss: 1.6026\n",
            "Epoch: 5/5... Step: 11200... Loss: 1.4731... Val Loss: 1.6049\n",
            "Epoch: 5/5... Step: 11210... Loss: 1.4651... Val Loss: 1.6048\n",
            "Epoch: 5/5... Step: 11220... Loss: 1.4930... Val Loss: 1.6037\n",
            "Epoch: 5/5... Step: 11230... Loss: 1.5434... Val Loss: 1.6030\n",
            "Epoch: 5/5... Step: 11240... Loss: 1.5124... Val Loss: 1.6068\n",
            "Epoch: 5/5... Step: 11250... Loss: 1.4862... Val Loss: 1.6031\n",
            "Epoch: 5/5... Step: 11260... Loss: 1.4150... Val Loss: 1.6011\n",
            "Epoch: 5/5... Step: 11270... Loss: 1.4716... Val Loss: 1.6023\n",
            "Epoch: 5/5... Step: 11280... Loss: 1.5020... Val Loss: 1.6019\n",
            "Epoch: 5/5... Step: 11290... Loss: 1.4608... Val Loss: 1.6003\n",
            "Epoch: 5/5... Step: 11300... Loss: 1.5200... Val Loss: 1.6037\n",
            "Epoch: 5/5... Step: 11310... Loss: 1.4860... Val Loss: 1.6021\n",
            "Epoch: 5/5... Step: 11320... Loss: 1.4956... Val Loss: 1.6049\n",
            "Epoch: 5/5... Step: 11330... Loss: 1.4877... Val Loss: 1.6036\n",
            "Epoch: 5/5... Step: 11340... Loss: 1.4252... Val Loss: 1.6011\n",
            "Epoch: 5/5... Step: 11350... Loss: 1.5442... Val Loss: 1.6012\n",
            "Epoch: 5/5... Step: 11360... Loss: 1.4447... Val Loss: 1.6061\n",
            "Epoch: 5/5... Step: 11370... Loss: 1.4534... Val Loss: 1.6022\n",
            "Epoch: 5/5... Step: 11380... Loss: 1.4716... Val Loss: 1.6012\n",
            "Epoch: 5/5... Step: 11390... Loss: 1.5789... Val Loss: 1.6004\n",
            "Epoch: 5/5... Step: 11400... Loss: 1.5173... Val Loss: 1.5990\n",
            "Epoch: 5/5... Step: 11410... Loss: 1.5250... Val Loss: 1.5999\n",
            "Epoch: 5/5... Step: 11420... Loss: 1.4832... Val Loss: 1.6012\n",
            "Epoch: 5/5... Step: 11430... Loss: 1.4758... Val Loss: 1.5979\n",
            "Epoch: 5/5... Step: 11440... Loss: 1.4808... Val Loss: 1.6012\n",
            "Epoch: 5/5... Step: 11450... Loss: 1.5030... Val Loss: 1.6079\n",
            "Epoch: 5/5... Step: 11460... Loss: 1.5081... Val Loss: 1.5990\n",
            "Epoch: 5/5... Step: 11470... Loss: 1.4650... Val Loss: 1.6002\n",
            "Epoch: 5/5... Step: 11480... Loss: 1.5447... Val Loss: 1.6025\n",
            "Epoch: 5/5... Step: 11490... Loss: 1.4979... Val Loss: 1.5994\n",
            "Epoch: 5/5... Step: 11500... Loss: 1.5102... Val Loss: 1.6022\n",
            "Epoch: 5/5... Step: 11510... Loss: 1.4511... Val Loss: 1.6017\n",
            "Epoch: 5/5... Step: 11520... Loss: 1.4383... Val Loss: 1.5995\n",
            "Epoch: 5/5... Step: 11530... Loss: 1.5369... Val Loss: 1.5977\n",
            "Epoch: 5/5... Step: 11540... Loss: 1.4580... Val Loss: 1.6004\n",
            "Epoch: 5/5... Step: 11550... Loss: 1.4474... Val Loss: 1.6066\n",
            "Epoch: 5/5... Step: 11560... Loss: 1.5219... Val Loss: 1.5997\n",
            "Epoch: 5/5... Step: 11570... Loss: 1.5443... Val Loss: 1.6019\n",
            "Epoch: 5/5... Step: 11580... Loss: 1.3935... Val Loss: 1.5998\n",
            "Epoch: 5/5... Step: 11590... Loss: 1.5071... Val Loss: 1.6009\n",
            "Epoch: 5/5... Step: 11600... Loss: 1.4679... Val Loss: 1.6004\n",
            "Epoch: 5/5... Step: 11610... Loss: 1.4316... Val Loss: 1.5956\n",
            "Epoch: 5/5... Step: 11620... Loss: 1.5060... Val Loss: 1.5972\n",
            "Epoch: 5/5... Step: 11630... Loss: 1.4926... Val Loss: 1.6003\n",
            "Epoch: 5/5... Step: 11640... Loss: 1.5051... Val Loss: 1.5972\n",
            "Epoch: 5/5... Step: 11650... Loss: 1.4819... Val Loss: 1.5993\n",
            "Epoch: 5/5... Step: 11660... Loss: 1.4647... Val Loss: 1.5993\n",
            "Epoch: 5/5... Step: 11670... Loss: 1.4879... Val Loss: 1.5999\n",
            "Epoch: 5/5... Step: 11680... Loss: 1.5711... Val Loss: 1.5982\n",
            "Epoch: 5/5... Step: 11690... Loss: 1.5167... Val Loss: 1.6012\n",
            "Epoch: 5/5... Step: 11700... Loss: 1.4153... Val Loss: 1.6028\n",
            "Epoch: 5/5... Step: 11710... Loss: 1.4090... Val Loss: 1.5979\n",
            "Epoch: 5/5... Step: 11720... Loss: 1.5091... Val Loss: 1.5983\n",
            "Epoch: 5/5... Step: 11730... Loss: 1.4500... Val Loss: 1.6054\n",
            "Epoch: 5/5... Step: 11740... Loss: 1.4961... Val Loss: 1.5980\n",
            "Epoch: 5/5... Step: 11750... Loss: 1.4323... Val Loss: 1.5980\n",
            "Epoch: 5/5... Step: 11760... Loss: 1.5616... Val Loss: 1.6027\n",
            "Epoch: 5/5... Step: 11770... Loss: 1.4105... Val Loss: 1.5980\n",
            "Epoch: 5/5... Step: 11780... Loss: 1.4925... Val Loss: 1.5987\n",
            "Epoch: 5/5... Step: 11790... Loss: 1.4444... Val Loss: 1.6065\n",
            "Epoch: 5/5... Step: 11800... Loss: 1.4859... Val Loss: 1.6034\n",
            "Epoch: 5/5... Step: 11810... Loss: 1.4723... Val Loss: 1.6013\n",
            "Epoch: 5/5... Step: 11820... Loss: 1.5360... Val Loss: 1.5996\n",
            "Epoch: 5/5... Step: 11830... Loss: 1.5168... Val Loss: 1.6011\n",
            "Epoch: 5/5... Step: 11840... Loss: 1.5366... Val Loss: 1.5989\n",
            "Epoch: 5/5... Step: 11850... Loss: 1.4101... Val Loss: 1.5986\n",
            "Epoch: 5/5... Step: 11860... Loss: 1.4056... Val Loss: 1.6012\n",
            "Epoch: 5/5... Step: 11870... Loss: 1.4944... Val Loss: 1.6006\n",
            "Epoch: 5/5... Step: 11880... Loss: 1.4284... Val Loss: 1.5961\n",
            "Epoch: 5/5... Step: 11890... Loss: 1.5439... Val Loss: 1.5998\n",
            "Epoch: 5/5... Step: 11900... Loss: 1.5156... Val Loss: 1.5961\n",
            "Epoch: 5/5... Step: 11910... Loss: 1.5041... Val Loss: 1.5961\n",
            "Epoch: 5/5... Step: 11920... Loss: 1.4703... Val Loss: 1.6034\n",
            "Epoch: 5/5... Step: 11930... Loss: 1.4942... Val Loss: 1.5988\n",
            "Epoch: 5/5... Step: 11940... Loss: 1.5125... Val Loss: 1.5984\n",
            "Epoch: 5/5... Step: 11950... Loss: 1.5535... Val Loss: 1.5990\n",
            "Epoch: 5/5... Step: 11960... Loss: 1.4827... Val Loss: 1.5986\n",
            "Epoch: 5/5... Step: 11970... Loss: 1.4259... Val Loss: 1.5988\n",
            "Epoch: 5/5... Step: 11980... Loss: 1.4923... Val Loss: 1.5966\n",
            "Epoch: 5/5... Step: 11990... Loss: 1.4901... Val Loss: 1.6019\n",
            "Epoch: 5/5... Step: 12000... Loss: 1.4022... Val Loss: 1.6026\n",
            "Epoch: 5/5... Step: 12010... Loss: 1.4678... Val Loss: 1.5958\n",
            "Epoch: 5/5... Step: 12020... Loss: 1.4706... Val Loss: 1.5980\n",
            "Epoch: 5/5... Step: 12030... Loss: 1.5422... Val Loss: 1.5962\n",
            "Epoch: 5/5... Step: 12040... Loss: 1.5192... Val Loss: 1.6039\n",
            "Epoch: 5/5... Step: 12050... Loss: 1.4424... Val Loss: 1.5960\n",
            "Epoch: 5/5... Step: 12060... Loss: 1.4864... Val Loss: 1.6009\n",
            "Epoch: 5/5... Step: 12070... Loss: 1.4939... Val Loss: 1.5988\n",
            "Epoch: 5/5... Step: 12080... Loss: 1.4739... Val Loss: 1.5980\n",
            "Epoch: 5/5... Step: 12090... Loss: 1.4584... Val Loss: 1.5968\n",
            "Epoch: 5/5... Step: 12100... Loss: 1.4985... Val Loss: 1.6001\n",
            "Epoch: 5/5... Step: 12110... Loss: 1.5347... Val Loss: 1.6021\n",
            "Epoch: 5/5... Step: 12120... Loss: 1.5882... Val Loss: 1.5943\n",
            "Epoch: 5/5... Step: 12130... Loss: 1.5422... Val Loss: 1.5951\n",
            "Epoch: 5/5... Step: 12140... Loss: 1.4566... Val Loss: 1.5965\n",
            "Epoch: 5/5... Step: 12150... Loss: 1.4721... Val Loss: 1.5970\n",
            "Epoch: 5/5... Step: 12160... Loss: 1.4495... Val Loss: 1.6008\n",
            "Epoch: 5/5... Step: 12170... Loss: 1.4213... Val Loss: 1.5969\n",
            "Epoch: 5/5... Step: 12180... Loss: 1.5056... Val Loss: 1.5965\n",
            "Epoch: 5/5... Step: 12190... Loss: 1.4415... Val Loss: 1.6010\n",
            "Epoch: 5/5... Step: 12200... Loss: 1.4654... Val Loss: 1.5955\n",
            "Epoch: 5/5... Step: 12210... Loss: 1.5357... Val Loss: 1.5955\n",
            "Epoch: 5/5... Step: 12220... Loss: 1.5496... Val Loss: 1.5982\n",
            "Epoch: 5/5... Step: 12230... Loss: 1.4278... Val Loss: 1.5911\n",
            "Epoch: 5/5... Step: 12240... Loss: 1.4929... Val Loss: 1.5969\n",
            "Epoch: 5/5... Step: 12250... Loss: 1.3541... Val Loss: 1.5966\n",
            "Epoch: 5/5... Step: 12260... Loss: 1.4546... Val Loss: 1.5923\n",
            "Epoch: 5/5... Step: 12270... Loss: 1.4365... Val Loss: 1.5938\n",
            "Epoch: 5/5... Step: 12280... Loss: 1.4670... Val Loss: 1.5956\n",
            "Epoch: 5/5... Step: 12290... Loss: 1.4346... Val Loss: 1.5926\n",
            "Epoch: 5/5... Step: 12300... Loss: 1.5411... Val Loss: 1.5954\n",
            "Epoch: 5/5... Step: 12310... Loss: 1.5295... Val Loss: 1.5949\n",
            "Epoch: 5/5... Step: 12320... Loss: 1.5712... Val Loss: 1.5932\n",
            "Epoch: 5/5... Step: 12330... Loss: 1.4381... Val Loss: 1.5929\n",
            "Epoch: 5/5... Step: 12340... Loss: 1.5084... Val Loss: 1.5986\n",
            "Epoch: 5/5... Step: 12350... Loss: 1.5148... Val Loss: 1.5927\n",
            "Epoch: 5/5... Step: 12360... Loss: 1.4758... Val Loss: 1.5937\n",
            "Epoch: 5/5... Step: 12370... Loss: 1.4898... Val Loss: 1.5959\n",
            "Epoch: 5/5... Step: 12380... Loss: 1.4321... Val Loss: 1.5962\n",
            "Epoch: 5/5... Step: 12390... Loss: 1.4950... Val Loss: 1.5932\n",
            "Epoch: 5/5... Step: 12400... Loss: 1.4483... Val Loss: 1.5966\n",
            "Epoch: 5/5... Step: 12410... Loss: 1.4918... Val Loss: 1.5939\n",
            "Epoch: 5/5... Step: 12420... Loss: 1.5247... Val Loss: 1.5928\n",
            "Epoch: 5/5... Step: 12430... Loss: 1.4982... Val Loss: 1.5956\n",
            "Epoch: 5/5... Step: 12440... Loss: 1.5577... Val Loss: 1.5955\n",
            "Epoch: 5/5... Step: 12450... Loss: 1.3555... Val Loss: 1.5939\n",
            "Epoch: 5/5... Step: 12460... Loss: 1.3928... Val Loss: 1.6027\n",
            "Epoch: 5/5... Step: 12470... Loss: 1.5056... Val Loss: 1.5922\n",
            "Epoch: 5/5... Step: 12480... Loss: 1.4613... Val Loss: 1.5964\n",
            "Epoch: 5/5... Step: 12490... Loss: 1.4701... Val Loss: 1.5927\n",
            "Epoch: 5/5... Step: 12500... Loss: 1.5148... Val Loss: 1.5951\n",
            "Epoch: 5/5... Step: 12510... Loss: 1.5059... Val Loss: 1.5919\n",
            "Epoch: 5/5... Step: 12520... Loss: 1.4768... Val Loss: 1.5919\n",
            "Epoch: 5/5... Step: 12530... Loss: 1.5072... Val Loss: 1.5979\n",
            "Epoch: 5/5... Step: 12540... Loss: 1.4261... Val Loss: 1.5946\n",
            "Epoch: 5/5... Step: 12550... Loss: 1.4397... Val Loss: 1.5939\n",
            "Epoch: 5/5... Step: 12560... Loss: 1.3623... Val Loss: 1.5973\n",
            "Epoch: 5/5... Step: 12570... Loss: 1.3883... Val Loss: 1.5925\n",
            "Epoch: 5/5... Step: 12580... Loss: 1.4562... Val Loss: 1.5916\n",
            "Epoch: 5/5... Step: 12590... Loss: 1.4092... Val Loss: 1.5935\n",
            "Epoch: 5/5... Step: 12600... Loss: 1.4849... Val Loss: 1.5892\n",
            "Epoch: 5/5... Step: 12610... Loss: 1.4846... Val Loss: 1.5918\n",
            "Epoch: 5/5... Step: 12620... Loss: 1.4696... Val Loss: 1.5941\n",
            "Epoch: 5/5... Step: 12630... Loss: 1.4809... Val Loss: 1.5920\n",
            "Epoch: 5/5... Step: 12640... Loss: 1.4416... Val Loss: 1.5966\n",
            "Epoch: 5/5... Step: 12650... Loss: 1.5220... Val Loss: 1.5981\n",
            "Epoch: 5/5... Step: 12660... Loss: 1.3582... Val Loss: 1.5900\n",
            "Epoch: 5/5... Step: 12670... Loss: 1.4039... Val Loss: 1.5961\n",
            "Epoch: 5/5... Step: 12680... Loss: 1.3471... Val Loss: 1.5921\n",
            "Epoch: 5/5... Step: 12690... Loss: 1.4955... Val Loss: 1.5904\n",
            "Epoch: 5/5... Step: 12700... Loss: 1.4044... Val Loss: 1.5925\n",
            "Epoch: 5/5... Step: 12710... Loss: 1.4341... Val Loss: 1.5956\n",
            "Epoch: 5/5... Step: 12720... Loss: 1.4721... Val Loss: 1.5913\n",
            "Epoch: 5/5... Step: 12730... Loss: 1.4316... Val Loss: 1.5940\n",
            "Epoch: 5/5... Step: 12740... Loss: 1.4877... Val Loss: 1.5919\n",
            "Epoch: 5/5... Step: 12750... Loss: 1.4928... Val Loss: 1.5876\n",
            "Epoch: 5/5... Step: 12760... Loss: 1.4649... Val Loss: 1.5895\n",
            "Epoch: 5/5... Step: 12770... Loss: 1.5114... Val Loss: 1.5877\n",
            "Epoch: 5/5... Step: 12780... Loss: 1.4966... Val Loss: 1.5850\n",
            "Epoch: 5/5... Step: 12790... Loss: 1.4407... Val Loss: 1.5879\n",
            "Epoch: 5/5... Step: 12800... Loss: 1.4407... Val Loss: 1.5913\n",
            "Epoch: 5/5... Step: 12810... Loss: 1.4956... Val Loss: 1.5881\n",
            "Epoch: 5/5... Step: 12820... Loss: 1.4883... Val Loss: 1.5865\n",
            "Epoch: 5/5... Step: 12830... Loss: 1.5385... Val Loss: 1.5904\n",
            "Epoch: 5/5... Step: 12840... Loss: 1.4599... Val Loss: 1.5868\n",
            "Epoch: 5/5... Step: 12850... Loss: 1.5043... Val Loss: 1.5880\n",
            "Epoch: 5/5... Step: 12860... Loss: 1.4777... Val Loss: 1.5865\n",
            "Epoch: 5/5... Step: 12870... Loss: 1.5126... Val Loss: 1.5856\n",
            "Epoch: 5/5... Step: 12880... Loss: 1.5346... Val Loss: 1.5860\n",
            "Epoch: 5/5... Step: 12890... Loss: 1.4226... Val Loss: 1.5896\n",
            "Epoch: 5/5... Step: 12900... Loss: 1.5414... Val Loss: 1.5854\n",
            "Epoch: 5/5... Step: 12910... Loss: 1.4427... Val Loss: 1.5886\n",
            "Epoch: 5/5... Step: 12920... Loss: 1.5019... Val Loss: 1.5893\n",
            "Epoch: 5/5... Step: 12930... Loss: 1.6494... Val Loss: 1.5874\n",
            "Epoch: 5/5... Step: 12940... Loss: 1.5001... Val Loss: 1.5873\n",
            "Epoch: 5/5... Step: 12950... Loss: 1.5687... Val Loss: 1.5866\n",
            "Epoch: 5/5... Step: 12960... Loss: 1.5397... Val Loss: 1.5837\n",
            "Epoch: 5/5... Step: 12970... Loss: 1.5622... Val Loss: 1.5874\n",
            "Epoch: 5/5... Step: 12980... Loss: 1.5161... Val Loss: 1.5864\n",
            "Epoch: 5/5... Step: 12990... Loss: 1.5019... Val Loss: 1.5873\n",
            "Epoch: 5/5... Step: 13000... Loss: 1.4724... Val Loss: 1.5885\n",
            "Epoch: 5/5... Step: 13010... Loss: 1.5533... Val Loss: 1.5922\n",
            "Epoch: 5/5... Step: 13020... Loss: 1.5275... Val Loss: 1.5872\n",
            "Epoch: 5/5... Step: 13030... Loss: 1.3956... Val Loss: 1.5857\n",
            "Epoch: 5/5... Step: 13040... Loss: 1.5139... Val Loss: 1.5881\n",
            "Epoch: 5/5... Step: 13050... Loss: 1.5121... Val Loss: 1.5875\n",
            "Epoch: 5/5... Step: 13060... Loss: 1.4024... Val Loss: 1.5867\n",
            "Epoch: 5/5... Step: 13070... Loss: 1.6014... Val Loss: 1.5861\n",
            "Epoch: 5/5... Step: 13080... Loss: 2.3067... Val Loss: 1.5854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_next_states_and_output"
      ],
      "metadata": {
        "id": "DohxYFW6DDH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cell_state, hidden_state = language_model.get_next_states_and_output('ب')\n",
        "print(cell_state)\n",
        "print()\n",
        "print(hidden_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iGSt5oM9RtR",
        "outputId": "be48cf33-eab9-4300-a79b-13f3f9661703"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0056, -0.0732,  0.2030, -0.0972,  0.1719,  0.2645,  0.0385,  0.0737,\n",
            "         -0.0653, -0.0183, -0.0580,  0.0734, -0.2100,  0.0750, -0.1753, -0.2902,\n",
            "          0.1157, -0.0907,  0.0802, -0.0352, -0.1189, -0.3811, -0.1422, -0.1905,\n",
            "         -0.0934, -0.2010, -0.0696, -0.3466, -0.0700,  0.0831, -0.1686,  0.1387,\n",
            "         -0.0587,  0.3436, -0.2360,  0.0078, -0.2036, -0.0653,  0.2795, -0.1667,\n",
            "         -0.3319,  0.1386,  0.1052, -0.0821,  0.2270, -0.1388,  0.1105,  0.0580,\n",
            "         -0.0231, -0.0223, -0.0975, -0.0202,  0.0825,  0.0660, -0.0474, -0.2568,\n",
            "          0.0597,  0.1290,  0.3529, -0.1590,  0.1554,  0.0433,  0.0350, -0.0501,\n",
            "          0.0112,  0.0717,  0.0158, -0.1837,  0.2491,  0.1181,  0.0098, -0.1834,\n",
            "         -0.2348, -0.1566, -0.0012, -0.2863, -0.0137, -0.1781,  0.0146, -0.0363,\n",
            "         -0.1351, -0.2516,  0.0012, -0.1720, -0.5492, -0.1447, -0.0874,  0.0543,\n",
            "          0.0971, -0.1577,  0.2378, -0.0748,  0.0170]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "(tensor([[[-0.0003,  0.0087, -0.0006,  ...,  0.0123,  0.0037,  0.0144]],\n",
            "\n",
            "        [[-0.0146, -0.0166,  0.0009,  ...,  0.0054, -0.0082,  0.0009]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>), tensor([[[-0.0006,  0.0170, -0.0012,  ...,  0.0245,  0.0075,  0.0287]],\n",
            "\n",
            "        [[-0.0295, -0.0327,  0.0019,  ...,  0.0107, -0.0161,  0.0017]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert_prefix_to_hiddens"
      ],
      "metadata": {
        "id": "wWF3obOcC_oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hiddens = language_model.convert_prefix_to_hiddens(prefix='انتخا')\n",
        "print(hiddens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9EYvAGl9tkQ",
        "outputId": "acae1377-8859-472b-f4a9-4f2a4d764643"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor([[[-0.0451, -0.0046,  0.0578,  ...,  0.0198, -0.0254,  0.0128]],\n",
            "\n",
            "        [[-0.0396,  0.1596, -0.0052,  ..., -0.0539,  0.0951, -0.0335]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>), tensor([[[-0.0854, -0.0077,  0.1269,  ...,  0.0571, -0.0607,  0.0412]],\n",
            "\n",
            "        [[-0.0854,  0.2100, -0.0334,  ..., -0.1234,  0.2649, -0.1666]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)), (tensor([[[-0.0192, -0.0322, -0.0033,  ...,  0.0330, -0.0006,  0.0329]],\n",
            "\n",
            "        [[ 0.1082, -0.0038, -0.0511,  ..., -0.0226,  0.0195,  0.0591]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>), tensor([[[-0.0581, -0.0476, -0.0104,  ...,  0.1378, -0.0050,  0.1219]],\n",
            "\n",
            "        [[ 0.3407, -0.0198, -0.2660,  ..., -0.0719,  0.0508,  0.3274]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)), (tensor([[[ 0.0567, -0.0723,  0.0059,  ...,  0.1324,  0.0395,  0.0717]],\n",
            "\n",
            "        [[ 0.0786, -0.0101, -0.0153,  ..., -0.0058, -0.0051,  0.0185]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>), tensor([[[ 0.1008, -0.1312,  0.0270,  ...,  0.2186,  0.0464,  0.2872]],\n",
            "\n",
            "        [[ 0.2262, -0.0762, -0.1209,  ..., -0.0164, -0.0271,  0.1225]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)), (tensor([[[-0.0272, -0.1199,  0.0146,  ..., -0.0545,  0.0518,  0.1499]],\n",
            "\n",
            "        [[ 0.1233, -0.0041, -0.0036,  ...,  0.4099,  0.0008, -0.0156]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>), tensor([[[-0.0735, -0.2336,  0.0583,  ..., -0.1752,  0.0679,  0.3096]],\n",
            "\n",
            "        [[ 0.2651, -0.0558, -0.0188,  ...,  0.6467,  0.0019, -0.0641]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)), (tensor([[[ 0.1142, -0.0601,  0.1044,  ..., -0.0163,  0.0699,  0.0227]],\n",
            "\n",
            "        [[-0.0433,  0.0786, -0.0297,  ...,  0.1719,  0.0614, -0.0227]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>), tensor([[[ 0.1803, -0.0843,  0.2653,  ..., -0.0445,  0.1031,  0.0543]],\n",
            "\n",
            "        [[-0.0887,  0.1273, -0.2164,  ...,  0.2213,  0.2687, -0.0707]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward0>))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 samples from get_probs"
      ],
      "metadata": {
        "id": "xGr9AnMzAFhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = language_model.get_probs(prefix='انتخا')\n",
        "p2 = language_model.get_probs(prefix='امتحا')\n",
        "p3 = language_model.get_probs(prefix='در طول سال')\n",
        "\n",
        "print(p1)\n",
        "print(p2)\n",
        "print(p3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WEmIl9RAGEo",
        "outputId": "51acad6b-21df-4160-b8d0-b5787423a8ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'e': 0.014001585, 'ﻧ': 0.008043705, 'ﺨ': 0.009812635, 'ض': 0.008001358, ' ': 0.007608446, 'ء': 0.014533796, 'غ': 0.0050784545, 'ژ': 0.009847666, 'ی': 0.005795712, 'ﺘ': 0.014476563, 'ع': 0.007929613, 'ك': 0.0104936175, 'ﻣ': 0.012922357, 'ة': 0.015813824, 'ﺸ': 0.004292164, '\\\\': 0.008525501, 'ﺑ': 0.0071774935, 'د': 0.014703526, 'ﺟ': 0.012297391, 'ﺣ': 0.005877054, 'ق': 0.012397284, 'ذ': 0.008965313, 'ﮔ': 0.008165866, 'N': 0.007774644, 'ﻌ': 0.008102357, 'ى': 0.011933966, '؟': 0.0059410203, '_': 0.008470178, 'ﻦ': 0.011244755, 'ö': 0.0074498365, 'ﻪ': 0.014633584, 'و': 0.011743731, 'ﺮ': 0.00716506, 'ﻲ': 0.0142216105, 'ﺎ': 0.0122018885, 'ﭼ': 0.019577848, 'ت': 0.00781299, 'ک': 0.0075172526, 'ۆ': 0.01468156, 'إ': 0.01091248, 'ä': 0.009198329, 'ز': 0.010382653, 'ﺼ': 0.014356847, 'ﯿ': 0.013576012, 'ظ': 0.034042377, 'ﮐ': 0.00905372, 'ش': 0.018376958, 'أ': 0.01603664, 'ﻳ': 0.011256437, 'ئ': 0.009126205, '.': 0.008775721, 'ل': 0.010402347, 'ط': 0.014709812, 'è': 0.0049991906, 'خ': 0.0049866303, 'م': 0.0040648575, 'ﻨ': 0.013042595, 'ح': 0.011652793, 'ﻘ': 0.017388692, 'پ': 0.014470513, 'ﭘ': 0.011616284, 'ﻮ': 0.017603375, 'ć': 0.014794769, 'ﺒ': 0.013659208, 'ﺶ': 0.008836202, 'ﻠ': 0.008817553, 'ؤ': 0.012931193, 's': 0.0074918005, 'ﺖ': 0.013566645, 'ۀ': 0.009610631, 'ر': 0.005907815, 'آ': 0.009866911, 'ج': 0.0056955484, 'ﺳ': 0.0061236275, 'ب': 0.013702251, 'س': 0.011029298, 'ه': 0.0140964305, 'ي': 0.006310485, 'ھ': 0.014475651, 'ﺴ': 0.011284665, 'ـ': 0.009143357, 'á': 0.0072318874, 'گ': 0.012761867, 'ن': 0.009809929, 'ف': 0.010351186, 'ث': 0.0055991597, 'چ': 0.008858422, '\\n': 0.01178097, 'ا': 0.0143965315, 'ﻓ': 0.008777909, 'ە': 0.008723948, 'ص': 0.008913115, 'ﺗ': 0.008186371}\n",
            "{'e': 0.013363762, 'ﻧ': 0.011694562, 'ﺨ': 0.0138424635, 'ض': 0.0056903968, ' ': 0.019270303, 'ء': 0.013904292, 'غ': 0.0048292466, 'ژ': 0.0087823, 'ی': 0.0056826198, 'ﺘ': 0.01113411, 'ع': 0.009296401, 'ك': 0.009241122, 'ﻣ': 0.013051845, 'ة': 0.01538121, 'ﺸ': 0.0074561373, '\\\\': 0.0077063804, 'ﺑ': 0.006599135, 'د': 0.010137876, 'ﺟ': 0.012988276, 'ﺣ': 0.007723915, 'ق': 0.005346076, 'ذ': 0.008127318, 'ﮔ': 0.007305448, 'N': 0.007951385, 'ﻌ': 0.0049972977, 'ى': 0.012448117, '؟': 0.0053629032, '_': 0.01093944, 'ﻦ': 0.017699078, 'ö': 0.0069274073, 'ﻪ': 0.0077556116, 'و': 0.014374077, 'ﺮ': 0.007815645, 'ﻲ': 0.017219895, 'ﺎ': 0.013431606, 'ﭼ': 0.011706365, 'ت': 0.009798799, 'ک': 0.0056659253, 'ۆ': 0.015462536, 'إ': 0.00878895, 'ä': 0.006101144, 'ز': 0.009323955, 'ﺼ': 0.008003967, 'ﯿ': 0.014752606, 'ظ': 0.018213244, 'ﮐ': 0.008615762, 'ش': 0.027901111, 'أ': 0.0113254925, 'ﻳ': 0.010897675, 'ئ': 0.018094357, '.': 0.007959419, 'ل': 0.008980243, 'ط': 0.015965765, 'è': 0.009220266, 'خ': 0.00634968, 'م': 0.006272801, 'ﻨ': 0.00693784, 'ح': 0.011191527, 'ﻘ': 0.020211304, 'پ': 0.006670054, 'ﭘ': 0.008216546, 'ﻮ': 0.01488395, 'ć': 0.007550779, 'ﺒ': 0.017005762, 'ﺶ': 0.01572853, 'ﻠ': 0.010327222, 'ؤ': 0.016240995, 's': 0.009326487, 'ﺖ': 0.028085392, 'ۀ': 0.007816248, 'ر': 0.008648468, 'آ': 0.005061197, 'ج': 0.0054896222, 'ﺳ': 0.0036419109, 'ب': 0.016141554, 'س': 0.008112354, 'ه': 0.008320632, 'ي': 0.008403589, 'ھ': 0.018270168, 'ﺴ': 0.014020979, 'ـ': 0.01008531, 'á': 0.009115514, 'گ': 0.010224212, 'ن': 0.009797026, 'ف': 0.0057529137, 'ث': 0.006169283, 'چ': 0.00861125, '\\n': 0.009311029, 'ا': 0.018050754, 'ﻓ': 0.010154754, 'ە': 0.008451782, 'ص': 0.014063476, 'ﺗ': 0.009031821}\n",
            "{'e': 0.013465216, 'ﻧ': 0.015911393, 'ﺨ': 0.01651879, 'ض': 0.005350624, ' ': 0.014823289, 'ء': 0.011623203, 'غ': 0.006969807, 'ژ': 0.009920394, 'ی': 0.005736651, 'ﺘ': 0.013206221, 'ع': 0.0073575573, 'ك': 0.0119761145, 'ﻣ': 0.007915688, 'ة': 0.017851857, 'ﺸ': 0.005957673, '\\\\': 0.009214882, 'ﺑ': 0.0090930015, 'د': 0.01270117, 'ﺟ': 0.0074334047, 'ﺣ': 0.010543941, 'ق': 0.005756322, 'ذ': 0.0056542004, 'ﮔ': 0.009576468, 'N': 0.008648851, 'ﻌ': 0.00977539, 'ى': 0.0105142845, '؟': 0.0055661136, '_': 0.004094491, 'ﻦ': 0.015238275, 'ö': 0.009232624, 'ﻪ': 0.014594146, 'و': 0.014936981, 'ﺮ': 0.007980031, 'ﻲ': 0.020855704, 'ﺎ': 0.009754051, 'ﭼ': 0.020570397, 'ت': 0.006277502, 'ک': 0.0059295627, 'ۆ': 0.013091134, 'إ': 0.018991454, 'ä': 0.0068885093, 'ز': 0.007909768, 'ﺼ': 0.027916467, 'ﯿ': 0.010891786, 'ظ': 0.02063014, 'ﮐ': 0.0064814533, 'ش': 0.023503099, 'أ': 0.013814107, 'ﻳ': 0.01677229, 'ئ': 0.013119128, '.': 0.005581389, 'ل': 0.008135769, 'ط': 0.014077318, 'è': 0.008855402, 'خ': 0.006147274, 'م': 0.005478705, 'ﻨ': 0.013082149, 'ح': 0.019868487, 'ﻘ': 0.015137903, 'پ': 0.0034209776, 'ﭘ': 0.013776081, 'ﻮ': 0.012100243, 'ć': 0.010694855, 'ﺒ': 0.014537319, 'ﺶ': 0.010762604, 'ﻠ': 0.01254189, 'ؤ': 0.012405377, 's': 0.0076455506, 'ﺖ': 0.013764819, 'ۀ': 0.0073422715, 'ر': 0.0065886998, 'آ': 0.010987312, 'ج': 0.005625212, 'ﺳ': 0.004976402, 'ب': 0.012759906, 'س': 0.0071416865, 'ه': 0.01310954, 'ي': 0.016092952, 'ھ': 0.008761313, 'ﺴ': 0.0070914575, 'ـ': 0.008250477, 'á': 0.0067067672, 'گ': 0.01266441, 'ن': 0.009567851, 'ف': 0.0054549742, 'ث': 0.005112425, 'چ': 0.005366391, '\\n': 0.0044991397, 'ا': 0.012793672, 'ﻓ': 0.011313257, 'ە': 0.00813185, 'ص': 0.013230217, 'ﺗ': 0.009882152}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 samples from get_next_char"
      ],
      "metadata": {
        "id": "M8JMtglcCSUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next_char, _, _ = language_model.get_next_char(char=\"ب\")\n",
        "print(next_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChgKxNG1BKn4",
        "outputId": "0414ac5c-2dd7-40b4-892a-a09b18f91b98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ش\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_char, _, _ = language_model.get_next_char(char=\"ق\")\n",
        "print(next_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU4FR1RhCWSF",
        "outputId": "daeaeff4-714c-44cc-9bc2-f12d6003d415"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "و\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_char, _, _ = language_model.get_next_char(char=\"س\")\n",
        "print(next_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVyx2So4CWWQ",
        "outputId": "a4a6ef17-2885-4eb2-f279-153ed6dab962"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ظ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 samples from generate_text"
      ],
      "metadata": {
        "id": "xoft0b0GBwVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_model.generate_text(prefix='انتخا', size=10)"
      ],
      "metadata": {
        "id": "C2KwkektO0Rf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7ef9eec2-742d-400c-d730-bf7779e05618"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'انتخاب و ما را '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language_model.generate_text(prefix='شنا در این منطقه', size=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "icO-8yQnCYV9",
        "outputId": "0e1be7c5-6c91-4b96-871c-c8d5a6f6d1f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'شنا در این منطقه ای که در این بخشی ب'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language_model.generate_text(prefix='اگر در طول ترم درس میخواندی', size=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YUj9WuwQCYYn",
        "outputId": "13742207-b330-4de5-ee2b-d623d1ecfefa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'اگر در طول ترم درس میخواندیانی و مراجعه کنند و با تهمه شو'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 samples from get_overall_prob"
      ],
      "metadata": {
        "id": "d4G28GdKB1A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_model.get_overal_prob(sentence='چیپس از پفک خوشمزه تر است')"
      ],
      "metadata": {
        "id": "iM1l73gPQd6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba98370-0500-4959-cefc-47c2fb1ff8f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-159.13538233811025"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language_model.get_overal_prob(sentence='کتاب در مقابل من است')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L92vWMqWCf5P",
        "outputId": "1c280da3-e149-4304-edfc-7447172995fd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-124.78817348147595"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language_model.get_overal_prob(sentence='روز سردی خواهد بود')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDgcGr2_Cf8x",
        "outputId": "87bde968-5a5a-4c33-f2f8-974b2ef15dab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-109.76139120375979"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate"
      ],
      "metadata": {
        "id": "0zZIcUSvCoOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "character_error_rate = language_model.evaluate(test.iloc[73]['text'])\n",
        "print(character_error_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoOSvV9Gtvuf",
        "outputId": "620a8ff0-4da5-4a08-e351-dd9426e4dcc5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7030497592295345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wL0yauo8Lo78"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}